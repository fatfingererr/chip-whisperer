---
title: å•†å“æ–°èçˆ¬èŸ²åŠŸèƒ½å¯¦ç¾ç ”ç©¶
date: 2026-01-02
ticket: N/A
author: Claude Code
tags:
  - web-crawler
  - news-scraping
  - telegram-integration
  - commodity-news
  - automation
  - anti-bot-detection
status: completed
related_files:
  - src/bot/telegram_bot.py
  - src/bot/handlers.py
  - src/bot/config.py
  - src/core/data_fetcher.py
  - markets/symbols.txt
  - scripts/run_bot.py
last_updated: 2026-01-02
last_updated_by: Claude Code
---

# å•†å“æ–°èçˆ¬èŸ²åŠŸèƒ½å¯¦ç¾ç ”ç©¶

## ç ”ç©¶å•é¡Œ

å¦‚ä½•åœ¨ç¾æœ‰çš„ Chip Whisperer å°ˆæ¡ˆä¸­å¯¦ç¾ä¸€å€‹å•†å“æ–°èçˆ¬èŸ²åŠŸèƒ½ï¼Œéœ€æ±‚å¦‚ä¸‹ï¼š

### æ ¸å¿ƒéœ€æ±‚

1. **å®šæ™‚æŠ“å–**ï¼šæ¯ 5 åˆ†é˜ï¼ˆ+/- 5% éš¨æ©Ÿæ€§ï¼‰æ‹‰å– `https://tradingeconomics.com/stream?c=commodity` ç¶²ç«™
2. **é˜²æª¢æ¸¬æ©Ÿåˆ¶**ï¼šæ·»åŠ é “é»æ“ä½œï¼ˆå»¶é²ã€éš¨æ©ŸåŒ–ç­‰ï¼‰é¿å…è¢«æª¢æ¸¬ç‚º AI çˆ¬èŸ²
3. **æ™ºæ…§å„²å­˜**ï¼š
   - æ ¹æ“šæ–°èå…§å®¹å°‡è³‡æ–™ä¿å­˜åˆ° `markets/<å°æ‡‰å•†å“>/yyyymmdd.txt`
   - è‹¥æ²’æœ‰å°æ‡‰å•†å“å‰‡ä¸ä¿å­˜
   - è‹¥æ²’æœ‰æ—¥æœŸæª”æ¡ˆå‰‡è‡ªå‹•å‰µå»º
4. **ID ç®¡ç†**ï¼šæ¯å‰‡æ–°èåœ¨ç•¶å¤©æœ‰éå¢ IDï¼Œä¿å­˜è‹±æ–‡åŸæ–‡
5. **Telegram é€šçŸ¥**ï¼šå°‡è‹±æ–‡åŸæ–‡ç™¼é€åˆ° Telegram ç¾¤çµ„

---

## æ‘˜è¦

æœ¬ç ”ç©¶æ·±å…¥åˆ†æäº†ç¾æœ‰å°ˆæ¡ˆçš„æ¶æ§‹å’Œå¯å¾©ç”¨å…ƒä»¶ï¼Œæå‡ºäº†ä¸€å€‹å®Œæ•´çš„å•†å“æ–°èçˆ¬èŸ²å¯¦ç¾æ–¹æ¡ˆã€‚ä¸»è¦ç™¼ç¾åŒ…æ‹¬ï¼š

- **ç¾æœ‰å¯å¾©ç”¨å…ƒä»¶**ï¼šTelegram Bot è¨Šæ¯ç™¼é€æ©Ÿåˆ¶ã€BotConfig é…ç½®ç®¡ç†ã€Loguru æ—¥èªŒç³»çµ±ã€asyncio æ”¯æ´
- **å»ºè­°æ¶æ§‹**ï¼šåœ¨ `src/` ä¸‹æ–°å¢ `crawler/` æ¨¡çµ„ï¼ŒåŒ…å«æ–°èçˆ¬èŸ²ã€å•†å“æ˜ å°„ã€æª”æ¡ˆç®¡ç†ç­‰åŠŸèƒ½
- **æŠ€è¡“é¸å‹**ï¼šhttpx + BeautifulSoup4 + APSchedulerï¼Œæ”¯æ´ async/await å’Œé˜²çˆ¬èŸ²ç­–ç•¥
- **æ•´åˆæ–¹å¼**ï¼šèˆ‡ç¾æœ‰ Telegram Bot å…±äº« Application å¯¦ä¾‹ï¼Œä½¿ç”¨ `post_init` é‰¤å­å•Ÿå‹•çˆ¬èŸ²

**é ä¼°å¯¦ä½œæ™‚é–“**ï¼š4-6 å°æ™‚ï¼ˆåŒ…å«æ¸¬è©¦å’Œé™¤éŒ¯ï¼‰

---

## è©³ç´°ç ”ç©¶çµæœ

### 1. ç¾æœ‰ç¨‹å¼ç¢¼æ¶æ§‹åˆ†æ

#### 1.1 å°ˆæ¡ˆç›®éŒ„çµæ§‹

```
chip-whisperer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/          # Claude Agentï¼ˆMT5 å·¥å…·æ•´åˆï¼‰
â”‚   â”œâ”€â”€ bot/            # Telegram Bot æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ config.py   # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ handlers.py # è¨Šæ¯è™•ç†å™¨
â”‚   â”‚   â””â”€â”€ telegram_bot.py  # Bot ä¸»ç¨‹å¼
â”‚   â”œâ”€â”€ core/           # MT5 å®¢æˆ¶ç«¯èˆ‡è³‡æ–™æŠ“å–
â”‚   â”‚   â”œâ”€â”€ mt5_client.py
â”‚   â”‚   â”œâ”€â”€ mt5_config.py
â”‚   â”‚   â”œâ”€â”€ data_fetcher.py  # æ­·å²è³‡æ–™æŠ“å–
â”‚   â”‚   â””â”€â”€ sqlite_cache.py  # SQLite å¿«å–ç®¡ç†
â”‚   â””â”€â”€ visualization/  # è¦–è¦ºåŒ–æ¨¡çµ„
â”œâ”€â”€ markets/            # å•†å“è³‡æ–™ç›®éŒ„
â”‚   â”œâ”€â”€ Gold/
â”‚   â”œâ”€â”€ Silver/
â”‚   â”œâ”€â”€ Brent/
â”‚   â”œâ”€â”€ Wti/
â”‚   â””â”€â”€ symbols.txt     # å•†å“ç¬¦è™Ÿå°ç…§è¡¨
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_bot.py      # Bot å•Ÿå‹•è…³æœ¬
â”‚   â””â”€â”€ backfill_data.py  # è³‡æ–™å›å¡«è…³æœ¬
â””â”€â”€ requirements.txt
```

#### 1.2 Telegram Bot æ¶æ§‹

**æª”æ¡ˆ**: `src/bot/telegram_bot.py`ï¼ˆç¬¬ 29-195 è¡Œï¼‰

- **æ ¸å¿ƒé¡åˆ¥**: `TelegramBot`
- **é‹è¡Œæ¨¡å¼**: Polling æ¨¡å¼ï¼ˆ`run()` æ–¹æ³•ï¼Œç¬¬ 130-152 è¡Œï¼‰
- **ç”Ÿå‘½é€±æœŸé‰¤å­**:
  - `_post_init()`: Bot å•Ÿå‹•å¾Œå›èª¿ï¼ˆç¬¬ 81-95 è¡Œï¼‰
  - `_post_shutdown()`: Bot é—œé–‰å‰å›èª¿ï¼ˆç¬¬ 122-128 è¡Œï¼‰
- **Application**: ä½¿ç”¨ `python-telegram-bot` çš„ `Application.builder()`ï¼ˆç¬¬ 46-50 è¡Œï¼‰

**é—œéµç™¼ç¾**ï¼š
- Bot å·²æ”¯æ´ `async/await` æ¨¡å¼
- å¯åœ¨ `_post_init()` ä¸­å•Ÿå‹•èƒŒæ™¯ä»»å‹™ï¼ˆå¦‚çˆ¬èŸ²å®šæ™‚å™¨ï¼‰
- Application å¯¦ä¾‹å„²å­˜åœ¨ `self.application`ï¼Œå¯å…±äº«çµ¦å…¶ä»–æ¨¡çµ„

#### 1.3 é…ç½®ç®¡ç†æ©Ÿåˆ¶

**æª”æ¡ˆ**: `src/bot/config.py`ï¼ˆç¬¬ 14-107 è¡Œï¼‰

- **é…ç½®é¡åˆ¥**: `BotConfig` (dataclass)
- **ç’°å¢ƒè®Šæ•¸è¼‰å…¥**: `from_env()` é¡æ–¹æ³•ï¼ˆç¬¬ 38-94 è¡Œï¼‰
- **ç¾æœ‰é…ç½®é …**:
  - `telegram_bot_token`
  - `telegram_group_ids`
  - `anthropic_api_key`
  - `claude_model`
  - `debug`

**æ“´å……é»**ï¼šå¯åœ¨ `BotConfig` ä¸­æ–°å¢çˆ¬èŸ²ç›¸é—œé…ç½®ï¼ˆçˆ¬å–é–“éš”ã€ç›®æ¨™ç¾¤çµ„ç­‰ï¼‰

#### 1.4 è¨Šæ¯ç™¼é€æ©Ÿåˆ¶

**æª”æ¡ˆ**: `src/bot/handlers.py`ï¼ˆç¬¬ 194-280 è¡Œï¼‰

**è¨Šæ¯ç™¼é€æ–¹å¼**ï¼š
```python
await update.message.reply_text(response)
# æˆ–ç›´æ¥ç™¼é€åˆ°æŒ‡å®šç¾¤çµ„
await application.bot.send_message(
    chat_id=group_id,
    text=message
)
```

**é—œéµç™¼ç¾**ï¼š
- `telegram_bot.py` ç¬¬ 111-120 è¡Œå·²å±•ç¤ºå¦‚ä½•æ‰¹æ¬¡ç™¼é€è¨Šæ¯åˆ°å¤šå€‹ç¾¤çµ„
- å¯ç›´æ¥ä½¿ç”¨ `application.bot.send_message()` ç™¼é€æ–°èé€šçŸ¥

#### 1.5 markets/ ç›®éŒ„çµæ§‹

**æª”æ¡ˆ**: `markets/symbols.txt`ï¼ˆç¬¬ 1-25 è¡Œï¼‰

**ç¾æœ‰å•†å“æ¸…å–®**ï¼ˆå…± 20 å€‹ï¼‰ï¼š
```
ALUMINIUM -> Aluminium
BITCOIN -> Bitcoin
BRENT -> Brent
COPPER -> Copper
ETHEREUM -> Ethereum
GOLD -> Gold
LEAD -> Lead
PALLADIUM -> Palladium
PLATINUM -> Platinum
SILVER -> Silver
SOLANA -> Solana
WTI -> Wti
ZINC -> Zinc
# è¾²ç”¢å“ï¼ˆå·²è¨»è§£ï¼‰ï¼š
# Cocoa_H26 -> Cocoa
# Coffee_H26 -> Coffee
# Corn_H26 -> Corn
# Cotton_H26 -> Cotton
# SBean_H26 -> Sbean
# Sugar_H26 -> Sugar
# Wheat_H26 -> Wheat
```

**ç›®éŒ„çµæ§‹ç¯„ä¾‹**ï¼š
```
markets/Gold/
markets/Silver/
markets/Brent/
markets/Wti/
...ï¼ˆç›®å‰éƒ½æ˜¯ç©ºç›®éŒ„ï¼‰
```

**é—œéµç™¼ç¾**ï¼š
- éœ€è¦å»ºç«‹å•†å“åç¨±æ˜ å°„è¡¨ï¼ˆæ–°èä¸­çš„åç¨± â†’ markets/ ç›®éŒ„åï¼‰
- æª”æ¡ˆå‘½åæ ¼å¼éœ€çµ±ä¸€ï¼ˆå»ºè­° `yyyymmdd.txt`ï¼‰

#### 1.6 æ—¥èªŒç³»çµ±

**æª”æ¡ˆ**: `scripts/run_bot.py`ï¼ˆç¬¬ 30-64 è¡Œï¼‰

- **æ—¥èªŒåº«**: Loguru
- **è¼¸å‡ºæ–¹å¼**:
  - æ§åˆ¶å°è¼¸å‡ºï¼ˆå½©è‰²æ ¼å¼ï¼‰
  - æª”æ¡ˆè¼¸å‡ºï¼ˆ`logs/bot_{time:YYYY-MM-DD}.log`ï¼Œæ¯æ—¥è¼ªæ›ï¼Œä¿ç•™ 30 å¤©ï¼‰
- **æ—¥èªŒç´šåˆ¥**: å¯é€é `DEBUG` ç’°å¢ƒè®Šæ•¸æ§åˆ¶

**é—œéµç™¼ç¾**ï¼šçˆ¬èŸ²å¯ç›´æ¥ä½¿ç”¨ç¾æœ‰çš„ Loguru é…ç½®ï¼Œç„¡éœ€é¡å¤–è¨­å®š

---

### 2. æŠ€è¡“é¸å‹å»ºè­°

#### 2.1 Web çˆ¬èŸ²åº«

| å¥—ä»¶ | å„ªé» | ç¼ºé» | å»ºè­° |
|------|------|------|------|
| **httpx** | æ”¯æ´ async/awaitã€æ•ˆèƒ½å„ªç§€ã€API é¡ä¼¼ requests | éœ€æ‰‹å‹•è™•ç† cookies | âœ… **æ¨è–¦** |
| requests | æˆç†Ÿç©©å®šã€æ–‡æª”è±å¯Œ | ä¸æ”¯æ´ async | âŒ ä¸é©åˆ |
| aiohttp | å·²åœ¨ `requirements.txt`ï¼ˆç¬¬ 30 è¡Œï¼‰ | API è¼ƒè¤‡é›œ | âš ï¸ å¯ç”¨ |
| selenium | å¯åŸ·è¡Œ JavaScript | è³‡æºæ¶ˆè€—å¤§ã€æ…¢ | âŒ éåº¦è¨­è¨ˆ |

**æœ€çµ‚é¸æ“‡**ï¼š`httpx` + `httpx_socks`ï¼ˆè‹¥éœ€ä»£ç†ï¼‰

**ç†ç”±**ï¼š
- èˆ‡ç¾æœ‰ `aiohttp` å…±å­˜ï¼ˆä¸è¡çªï¼‰
- æ”¯æ´ async/awaitï¼Œèˆ‡ Telegram Bot æ•´åˆå®¹æ˜“
- è¼•é‡ç´šï¼Œé©åˆç°¡å–®çš„ HTML æŠ“å–

#### 2.2 HTML è§£æåº«

| å¥—ä»¶ | å„ªé» | ç¼ºé» | å»ºè­° |
|------|------|------|------|
| **BeautifulSoup4** | æ˜“ç”¨ã€å®¹éŒ¯èƒ½åŠ›å¼· | æ•ˆèƒ½ç¨æ…¢ | âœ… **æ¨è–¦** |
| lxml | æ•ˆèƒ½æœ€ä½³ | èªæ³•è¼ƒè¤‡é›œ | âš ï¸ å¯ç”¨ |
| html.parser | å…§å»º | åŠŸèƒ½è¼ƒå¼± | âŒ ä¸æ¨è–¦ |

**æœ€çµ‚é¸æ“‡**ï¼š`beautifulsoup4` + `lxml` (ä½œç‚º parser)

**ç†ç”±**ï¼š
- æ˜“æ–¼è§£æ HTML çµæ§‹
- å®¹éŒ¯èƒ½åŠ›å¼·ï¼ˆå³ä½¿ç¶²ç«™çµæ§‹æ”¹è®Šä¹Ÿèƒ½éƒ¨åˆ†è§£æï¼‰
- ç¤¾ç¾¤æ”¯æ´è‰¯å¥½

#### 2.3 å®šæ™‚ä»»å‹™å¯¦ç¾

| æ–¹æ¡ˆ | å„ªé» | ç¼ºé» | å»ºè­° |
|------|------|------|------|
| **APScheduler** | åŠŸèƒ½å®Œæ•´ã€æ”¯æ´å¤šç¨®è§¸ç™¼å™¨ã€èˆ‡ asyncio æ•´åˆ | éœ€é¡å¤–ä¾è³´ | âœ… **æ¨è–¦** |
| asyncio.create_task + sleep | è¼•é‡ç´šã€ç„¡é¡å¤–ä¾è³´ | éœ€æ‰‹å‹•ç®¡ç†é‡å•Ÿé‚è¼¯ | âš ï¸ å¯ç”¨ |
| Celery | åŠŸèƒ½å¼·å¤§ | éåº¦è¤‡é›œï¼ˆéœ€ Redis/RabbitMQï¼‰ | âŒ éåº¦è¨­è¨ˆ |

**æœ€çµ‚é¸æ“‡**ï¼š`APScheduler` (AsyncIOScheduler)

**ç†ç”±**ï¼š
- æ”¯æ´éš¨æ©ŸåŒ–é–“éš”ï¼ˆjitterï¼‰
- èˆ‡ `python-telegram-bot` çš„ asyncio æ•´åˆè‰¯å¥½
- å¯åœ¨ Bot çš„ç”Ÿå‘½é€±æœŸå…§å„ªé›…å•Ÿå‹•/é—œé–‰

**ç¯„ä¾‹ç¨‹å¼ç¢¼**ï¼š
```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger

scheduler = AsyncIOScheduler()
scheduler.add_job(
    crawl_news,
    trigger=IntervalTrigger(minutes=5, jitter=15),  # 5 Â± 0.25 åˆ†é˜ï¼ˆ5% éš¨æ©Ÿæ€§ï¼‰
    id='news_crawler'
)
scheduler.start()
```

#### 2.4 é˜²çˆ¬èŸ²ç­–ç•¥

| ç­–ç•¥ | å¯¦ç¾æ–¹å¼ | å„ªå…ˆç´š |
|------|----------|--------|
| **User-Agent è¼ªæ›** | éš¨æ©Ÿé¸æ“‡å¸¸è¦‹ç€è¦½å™¨ UA | ğŸ”´ å¿…è¦ |
| **è«‹æ±‚é–“éš”éš¨æ©ŸåŒ–** | APScheduler jitter + æ¯æ¬¡è«‹æ±‚å»¶é² | ğŸ”´ å¿…è¦ |
| **Headers å½è£** | æ·»åŠ  Referer, Accept-Language ç­‰ | ğŸŸ¡ æ¨è–¦ |
| **ä»£ç† IP** | é€é httpx-socks | ğŸŸ¢ é¸ç”¨ |
| **Cookie ç®¡ç†** | httpx.Client() è‡ªå‹•è™•ç† | ğŸŸ¡ æ¨è–¦ |

**å»ºè­°å¯¦ç¾**ï¼š

```python
import random
import asyncio

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
]

async def fetch_with_delay(url: str) -> str:
    """å¸¶å»¶é²çš„è«‹æ±‚"""
    # éš¨æ©Ÿå»¶é² 0.5-2 ç§’
    await asyncio.sleep(random.uniform(0.5, 2.0))

    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Referer': 'https://tradingeconomics.com/',
        'Connection': 'keep-alive'
    }

    async with httpx.AsyncClient() as client:
        response = await client.get(url, headers=headers, timeout=30.0)
        return response.text
```

---

### 3. å»ºè­°æ¶æ§‹è¨­è¨ˆ

#### 3.1 æ–°æ¨¡çµ„ç›®éŒ„çµæ§‹

```
src/
â”œâ”€â”€ crawler/                    # æ–°å¢çˆ¬èŸ²æ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # çˆ¬èŸ²é…ç½®
â”‚   â”œâ”€â”€ news_crawler.py        # æ–°èçˆ¬èŸ²æ ¸å¿ƒ
â”‚   â”œâ”€â”€ commodity_mapper.py    # å•†å“åç¨±æ˜ å°„
â”‚   â”œâ”€â”€ news_storage.py        # æ–°èå„²å­˜ç®¡ç†
â”‚   â””â”€â”€ scheduler.py           # å®šæ™‚ä»»å‹™ç®¡ç†
â””â”€â”€ bot/
    â””â”€â”€ telegram_bot.py        # ä¿®æ”¹ï¼šæ•´åˆçˆ¬èŸ²å•Ÿå‹•
```

#### 3.2 æ¨¡çµ„è·è²¬åŠƒåˆ†

##### 3.2.1 `crawler/config.py` - çˆ¬èŸ²é…ç½®

```python
"""
çˆ¬èŸ²é…ç½®æ¨¡çµ„

ç®¡ç†çˆ¬èŸ²ç›¸é—œçš„é…ç½®åƒæ•¸ã€‚
"""

from dataclasses import dataclass
from typing import List
import os
from dotenv import load_dotenv


@dataclass
class CrawlerConfig:
    """
    çˆ¬èŸ²é…ç½®è³‡æ–™é¡åˆ¥

    å±¬æ€§ï¼š
        target_url: ç›®æ¨™ç¶²ç«™ URL
        crawl_interval_minutes: çˆ¬å–é–“éš”ï¼ˆåˆ†é˜ï¼‰
        interval_jitter_seconds: é–“éš”éš¨æ©ŸåŒ–ç¯„åœï¼ˆç§’ï¼‰
        markets_dir: markets ç›®éŒ„è·¯å¾‘
        enabled: æ˜¯å¦å•Ÿç”¨çˆ¬èŸ²
        telegram_notify_groups: è¦é€šçŸ¥çš„ Telegram ç¾¤çµ„ ID åˆ—è¡¨
    """

    target_url: str
    crawl_interval_minutes: int
    interval_jitter_seconds: int
    markets_dir: str
    enabled: bool
    telegram_notify_groups: List[int]

    @classmethod
    def from_env(cls) -> 'CrawlerConfig':
        """
        å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥é…ç½®

        å›å‚³ï¼š
            CrawlerConfig å¯¦ä¾‹
        """
        load_dotenv()

        return cls(
            target_url=os.getenv(
                'CRAWLER_TARGET_URL',
                'https://tradingeconomics.com/stream?c=commodity'
            ),
            crawl_interval_minutes=int(os.getenv('CRAWLER_INTERVAL_MINUTES', '5')),
            interval_jitter_seconds=int(os.getenv('CRAWLER_JITTER_SECONDS', '15')),  # 5%
            markets_dir=os.getenv('MARKETS_DIR', 'markets'),
            enabled=os.getenv('CRAWLER_ENABLED', 'true').lower() in ('true', '1', 'yes'),
            telegram_notify_groups=[
                int(gid.strip())
                for gid in os.getenv('CRAWLER_NOTIFY_GROUPS', '').split(',')
                if gid.strip()
            ]
        )
```

##### 3.2.2 `crawler/commodity_mapper.py` - å•†å“åç¨±æ˜ å°„

```python
"""
å•†å“åç¨±æ˜ å°„æ¨¡çµ„

æä¾›æ–°èä¸­çš„å•†å“åç¨±èˆ‡ markets/ ç›®éŒ„çš„æ˜ å°„é—œä¿‚ã€‚
"""

from typing import Optional, Dict
from pathlib import Path
from loguru import logger


class CommodityMapper:
    """
    å•†å“åç¨±æ˜ å°„å™¨

    è² è²¬å°‡æ–°èä¸­çš„å•†å“åç¨±æ˜ å°„åˆ° markets/ ç›®éŒ„åç¨±ã€‚
    """

    # å•†å“åç¨±æ˜ å°„è¡¨ï¼ˆæ–°èé—œéµå­— -> markets ç›®éŒ„åï¼‰
    COMMODITY_MAP = {
        # è²´é‡‘å±¬
        'gold': 'Gold',
        'silver': 'Silver',
        'platinum': 'Platinum',
        'palladium': 'Palladium',

        # èƒ½æº
        'crude oil': 'Wti',
        'wti': 'Wti',
        'brent': 'Brent',
        'oil': 'Wti',  # é è¨­ç‚º WTI

        # åŸºæœ¬é‡‘å±¬
        'copper': 'Copper',
        'aluminium': 'Aluminium',
        'aluminum': 'Aluminium',  # ç¾å¼æ‹¼æ³•
        'zinc': 'Zinc',
        'lead': 'Lead',

        # åŠ å¯†è²¨å¹£
        'bitcoin': 'Bitcoin',
        'btc': 'Bitcoin',
        'ethereum': 'Ethereum',
        'eth': 'Ethereum',
        'solana': 'Solana',
        'sol': 'Solana',

        # è¾²ç”¢å“ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        'cocoa': 'Cocoa',
        'coffee': 'Coffee',
        'corn': 'Corn',
        'cotton': 'Cotton',
        'soybean': 'Sbean',
        'sugar': 'Sugar',
        'wheat': 'Wheat',
    }

    def __init__(self, markets_dir: str = 'markets'):
        """
        åˆå§‹åŒ–æ˜ å°„å™¨

        åƒæ•¸ï¼š
            markets_dir: markets ç›®éŒ„è·¯å¾‘
        """
        self.markets_dir = Path(markets_dir)
        self._load_available_commodities()

    def _load_available_commodities(self):
        """è¼‰å…¥ markets/ ç›®éŒ„ä¸‹å¯¦éš›å­˜åœ¨çš„å•†å“"""
        if not self.markets_dir.exists():
            logger.warning(f"markets ç›®éŒ„ä¸å­˜åœ¨ï¼š{self.markets_dir}")
            self.available_commodities = set()
            return

        # å–å¾—æ‰€æœ‰å­ç›®éŒ„åç¨±
        self.available_commodities = {
            d.name for d in self.markets_dir.iterdir()
            if d.is_dir() and not d.name.startswith('.')
        }

        logger.info(f"å·²è¼‰å…¥ {len(self.available_commodities)} å€‹å¯ç”¨å•†å“ç›®éŒ„")
        logger.debug(f"å¯ç”¨å•†å“ï¼š{sorted(self.available_commodities)}")

    def extract_commodity(self, news_text: str) -> Optional[str]:
        """
        å¾æ–°èæ–‡æœ¬ä¸­æå–å•†å“åç¨±

        åƒæ•¸ï¼š
            news_text: æ–°èæ–‡æœ¬ï¼ˆè‹±æ–‡ï¼‰

        å›å‚³ï¼š
            å•†å“ç›®éŒ„åç¨±ï¼ˆå¦‚ 'Gold'ï¼‰ï¼Œè‹¥ç„¡åŒ¹é…å‰‡å›å‚³ None
        """
        news_lower = news_text.lower()

        # æŒ‰æ˜ å°„è¡¨é€ä¸€æª¢æŸ¥
        for keyword, commodity_dir in self.COMMODITY_MAP.items():
            if keyword in news_lower:
                # æª¢æŸ¥è©²å•†å“ç›®éŒ„æ˜¯å¦å­˜åœ¨
                if commodity_dir in self.available_commodities:
                    logger.debug(f"åŒ¹é…å•†å“ï¼š{keyword} -> {commodity_dir}")
                    return commodity_dir
                else:
                    logger.debug(f"å•†å“ {commodity_dir} ç›®éŒ„ä¸å­˜åœ¨ï¼Œå¿½ç•¥")

        return None

    def is_valid_commodity(self, commodity_dir: str) -> bool:
        """
        æª¢æŸ¥å•†å“ç›®éŒ„æ˜¯å¦æœ‰æ•ˆ

        åƒæ•¸ï¼š
            commodity_dir: å•†å“ç›®éŒ„åç¨±

        å›å‚³ï¼š
            æ˜¯å¦æœ‰æ•ˆ
        """
        return commodity_dir in self.available_commodities
```

##### 3.2.3 `crawler/news_storage.py` - æ–°èå„²å­˜ç®¡ç†

```python
"""
æ–°èå„²å­˜æ¨¡çµ„

è² è²¬å°‡æ–°èä¿å­˜åˆ° markets/<å•†å“>/yyyymmdd.txtï¼Œä¸¦ç®¡ç† IDã€‚
"""

from typing import Optional, Tuple
from pathlib import Path
from datetime import datetime
import fcntl  # Unix/Linux æª”æ¡ˆé–ï¼ˆWindows éœ€ä½¿ç”¨ msvcrtï¼‰
from loguru import logger


class NewsStorage:
    """
    æ–°èå„²å­˜ç®¡ç†å™¨

    è² è²¬å°‡æ–°èä¿å­˜åˆ°å°æ‡‰å•†å“ç›®éŒ„ï¼Œä¸¦ç®¡ç†éå¢ IDã€‚
    """

    def __init__(self, markets_dir: str = 'markets'):
        """
        åˆå§‹åŒ–å„²å­˜ç®¡ç†å™¨

        åƒæ•¸ï¼š
            markets_dir: markets ç›®éŒ„è·¯å¾‘
        """
        self.markets_dir = Path(markets_dir)
        self.markets_dir.mkdir(parents=True, exist_ok=True)

    def save_news(
        self,
        commodity_dir: str,
        news_text: str,
        date: Optional[datetime] = None
    ) -> Tuple[bool, int]:
        """
        ä¿å­˜æ–°èåˆ°æŒ‡å®šå•†å“ç›®éŒ„

        åƒæ•¸ï¼š
            commodity_dir: å•†å“ç›®éŒ„åç¨±ï¼ˆå¦‚ 'Gold'ï¼‰
            news_text: æ–°èæ–‡æœ¬ï¼ˆè‹±æ–‡åŸæ–‡ï¼‰
            date: æ—¥æœŸï¼ˆé è¨­ç‚ºç•¶å¤©ï¼‰

        å›å‚³ï¼š
            (æ˜¯å¦æˆåŠŸ, æ–°è ID)
        """
        if date is None:
            date = datetime.now()

        # ç¢ºä¿å•†å“ç›®éŒ„å­˜åœ¨
        commodity_path = self.markets_dir / commodity_dir
        commodity_path.mkdir(parents=True, exist_ok=True)

        # æª”æ¡ˆè·¯å¾‘
        date_str = date.strftime('%Y%m%d')
        file_path = commodity_path / f"{date_str}.txt"

        # å–å¾—ä¸‹ä¸€å€‹ ID
        next_id = self._get_next_id(file_path)

        # å¯«å…¥æ–°èï¼ˆé™„åŠ æ¨¡å¼ï¼‰
        try:
            with open(file_path, 'a', encoding='utf-8') as f:
                # æª”æ¡ˆé–ï¼ˆé¿å…ä¸¦ç™¼å¯«å…¥ï¼‰
                try:
                    fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                except:
                    pass  # Windows ä¸æ”¯æ´ fcntlï¼Œå¿½ç•¥

                # å¯«å…¥æ ¼å¼ï¼š[ID] æ–°èå…§å®¹
                f.write(f"[{next_id}] {news_text}\n")
                f.write("-" * 80 + "\n")

            logger.info(f"æ–°èå·²ä¿å­˜ï¼š{file_path} (ID: {next_id})")
            return True, next_id

        except Exception as e:
            logger.error(f"ä¿å­˜æ–°èå¤±æ•—ï¼š{e}")
            return False, -1

    def _get_next_id(self, file_path: Path) -> int:
        """
        å–å¾—æª”æ¡ˆä¸­çš„ä¸‹ä¸€å€‹ ID

        åƒæ•¸ï¼š
            file_path: æª”æ¡ˆè·¯å¾‘

        å›å‚³ï¼š
            ä¸‹ä¸€å€‹ IDï¼ˆå¾ 1 é–‹å§‹ï¼‰
        """
        if not file_path.exists():
            return 1

        # è®€å–æª”æ¡ˆï¼Œè¨ˆç®—ç¾æœ‰ ID æ•¸é‡
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # çµ±è¨ˆ [ID] å‡ºç¾æ¬¡æ•¸
            id_count = sum(1 for line in lines if line.strip().startswith('['))
            return id_count + 1

        except Exception as e:
            logger.warning(f"è®€å–æª”æ¡ˆå¤±æ•—ï¼ŒID å¾ 1 é–‹å§‹ï¼š{e}")
            return 1

    def check_duplicate(
        self,
        commodity_dir: str,
        news_text: str,
        date: Optional[datetime] = None
    ) -> bool:
        """
        æª¢æŸ¥æ–°èæ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰

        åƒæ•¸ï¼š
            commodity_dir: å•†å“ç›®éŒ„åç¨±
            news_text: æ–°èæ–‡æœ¬
            date: æ—¥æœŸ

        å›å‚³ï¼š
            æ˜¯å¦é‡è¤‡
        """
        if date is None:
            date = datetime.now()

        date_str = date.strftime('%Y%m%d')
        file_path = self.markets_dir / commodity_dir / f"{date_str}.txt"

        if not file_path.exists():
            return False

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # ç°¡å–®çš„å­—ä¸²åŒ…å«æª¢æŸ¥ï¼ˆå¯æ”¹ç‚ºæ›´ç²¾ç¢ºçš„å»é‡é‚è¼¯ï¼‰
            return news_text.strip() in content

        except Exception as e:
            logger.warning(f"æª¢æŸ¥é‡è¤‡æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return False
```

##### 3.2.4 `crawler/news_crawler.py` - æ–°èçˆ¬èŸ²æ ¸å¿ƒ

```python
"""
æ–°èçˆ¬èŸ²æ ¸å¿ƒæ¨¡çµ„

è² è²¬å¾ç›®æ¨™ç¶²ç«™æŠ“å–å•†å“æ–°èã€‚
"""

from typing import List, Dict, Optional
import random
import asyncio
import httpx
from bs4 import BeautifulSoup
from loguru import logger
from datetime import datetime

from .config import CrawlerConfig
from .commodity_mapper import CommodityMapper
from .news_storage import NewsStorage


# User-Agent åˆ—è¡¨
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]


class NewsCrawler:
    """
    å•†å“æ–°èçˆ¬èŸ²

    è² è²¬å¾ tradingeconomics.com æŠ“å–å•†å“æ–°èã€‚
    """

    def __init__(self, config: CrawlerConfig):
        """
        åˆå§‹åŒ–çˆ¬èŸ²

        åƒæ•¸ï¼š
            config: çˆ¬èŸ²é…ç½®
        """
        self.config = config
        self.mapper = CommodityMapper(config.markets_dir)
        self.storage = NewsStorage(config.markets_dir)

        logger.info("æ–°èçˆ¬èŸ²åˆå§‹åŒ–å®Œæˆ")

    async def fetch_page(self) -> Optional[str]:
        """
        æŠ“å–ç›®æ¨™ç¶²é  HTML

        å›å‚³ï¼š
            HTML å…§å®¹ï¼Œå¤±æ•—æ™‚å›å‚³ None
        """
        # éš¨æ©Ÿå»¶é²ï¼ˆ0.5-2 ç§’ï¼‰
        await asyncio.sleep(random.uniform(0.5, 2.0))

        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://tradingeconomics.com/',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

        try:
            async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                logger.info(f"æ­£åœ¨æŠ“å–ï¼š{self.config.target_url}")
                response = await client.get(self.config.target_url, headers=headers)

                if response.status_code == 200:
                    logger.info("ç¶²é æŠ“å–æˆåŠŸ")
                    return response.text
                else:
                    logger.error(f"ç¶²é æŠ“å–å¤±æ•—ï¼šHTTP {response.status_code}")
                    return None

        except Exception as e:
            logger.error(f"ç¶²é æŠ“å–ç•°å¸¸ï¼š{e}")
            return None

    def parse_news(self, html: str) -> List[Dict[str, str]]:
        """
        è§£æ HTMLï¼Œæå–æ–°èåˆ—è¡¨

        åƒæ•¸ï¼š
            html: ç¶²é  HTML å…§å®¹

        å›å‚³ï¼š
            æ–°èåˆ—è¡¨ [{'title': ..., 'content': ..., 'time': ...}, ...]
        """
        soup = BeautifulSoup(html, 'lxml')
        news_list = []

        # TODO: æ ¹æ“šå¯¦éš›ç¶²ç«™çµæ§‹èª¿æ•´é¸æ“‡å™¨
        # ä»¥ä¸‹ç‚ºç¤ºæ„ç¨‹å¼ç¢¼ï¼Œéœ€æ ¹æ“š tradingeconomics.com çš„å¯¦éš› HTML çµæ§‹ä¿®æ”¹
        try:
            # ç¯„ä¾‹ï¼šå‡è¨­æ–°èåœ¨ <div class="stream-item"> ä¸­
            items = soup.select('div.stream-item')  # éœ€æ ¹æ“šå¯¦éš›èª¿æ•´

            for item in items:
                try:
                    # æå–æ¨™é¡Œ
                    title_elem = item.select_one('h3')  # éœ€æ ¹æ“šå¯¦éš›èª¿æ•´
                    title = title_elem.get_text(strip=True) if title_elem else ''

                    # æå–å…§å®¹
                    content_elem = item.select_one('p')  # éœ€æ ¹æ“šå¯¦éš›èª¿æ•´
                    content = content_elem.get_text(strip=True) if content_elem else ''

                    # æå–æ™‚é–“
                    time_elem = item.select_one('time')  # éœ€æ ¹æ“šå¯¦éš›èª¿æ•´
                    time_str = time_elem.get('datetime', '') if time_elem else ''

                    # çµ„åˆå®Œæ•´æ–‡æœ¬
                    full_text = f"{title}\n{content}" if content else title

                    if full_text:
                        news_list.append({
                            'title': title,
                            'content': content,
                            'full_text': full_text,
                            'time': time_str
                        })

                except Exception as e:
                    logger.warning(f"è§£æå–®å‰‡æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

            logger.info(f"æˆåŠŸè§£æ {len(news_list)} å‰‡æ–°è")

        except Exception as e:
            logger.error(f"è§£æ HTML æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

        return news_list

    async def process_and_save(
        self,
        news_list: List[Dict[str, str]]
    ) -> List[Dict[str, any]]:
        """
        è™•ç†æ–°èåˆ—è¡¨ä¸¦ä¿å­˜

        åƒæ•¸ï¼š
            news_list: è§£æå¾Œçš„æ–°èåˆ—è¡¨

        å›å‚³ï¼š
            å·²ä¿å­˜çš„æ–°èåˆ—è¡¨ï¼ˆåŒ…å«å•†å“å’Œ ID è³‡è¨Šï¼‰
        """
        saved_news = []

        for news in news_list:
            full_text = news['full_text']

            # æå–å•†å“
            commodity = self.mapper.extract_commodity(full_text)
            if not commodity:
                logger.debug(f"æ–°èæœªåŒ¹é…ä»»ä½•å•†å“ï¼Œå¿½ç•¥ï¼š{full_text[:50]}...")
                continue

            # æª¢æŸ¥é‡è¤‡
            if self.storage.check_duplicate(commodity, full_text):
                logger.debug(f"æ–°èé‡è¤‡ï¼Œå¿½ç•¥ï¼š{full_text[:50]}...")
                continue

            # ä¿å­˜æ–°è
            success, news_id = self.storage.save_news(commodity, full_text)

            if success:
                saved_news.append({
                    'commodity': commodity,
                    'news_id': news_id,
                    'text': full_text,
                    'time': news.get('time', '')
                })
                logger.info(f"æ–°èå·²ä¿å­˜ï¼š{commodity} ID={news_id}")

        return saved_news

    async def crawl(self) -> List[Dict[str, any]]:
        """
        åŸ·è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹

        å›å‚³ï¼š
            å·²ä¿å­˜çš„æ–°èåˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("é–‹å§‹çˆ¬å–å•†å“æ–°è")
        logger.info("=" * 60)

        # 1. æŠ“å–ç¶²é 
        html = await self.fetch_page()
        if not html:
            logger.error("ç¶²é æŠ“å–å¤±æ•—ï¼Œæœ¬æ¬¡çˆ¬å–çµæŸ")
            return []

        # 2. è§£ææ–°è
        news_list = self.parse_news(html)
        if not news_list:
            logger.warning("æœªè§£æåˆ°ä»»ä½•æ–°è")
            return []

        # 3. è™•ç†ä¸¦ä¿å­˜
        saved_news = await self.process_and_save(news_list)

        logger.info("=" * 60)
        logger.info(f"çˆ¬å–å®Œæˆï¼šå…±ä¿å­˜ {len(saved_news)} å‰‡æ–°è")
        logger.info("=" * 60)

        return saved_news
```

##### 3.2.5 `crawler/scheduler.py` - å®šæ™‚ä»»å‹™ç®¡ç†

```python
"""
çˆ¬èŸ²å®šæ™‚ä»»å‹™ç®¡ç†æ¨¡çµ„

è² è²¬å•Ÿå‹•å’Œç®¡ç†æ–°èçˆ¬èŸ²çš„å®šæ™‚ä»»å‹™ã€‚
"""

from typing import Optional
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from loguru import logger
from telegram.ext import Application

from .config import CrawlerConfig
from .news_crawler import NewsCrawler


class CrawlerScheduler:
    """
    çˆ¬èŸ²å®šæ™‚ä»»å‹™ç®¡ç†å™¨

    ä½¿ç”¨ APScheduler ç®¡ç†çˆ¬èŸ²çš„å®šæ™‚åŸ·è¡Œã€‚
    """

    def __init__(
        self,
        config: CrawlerConfig,
        telegram_app: Optional[Application] = None
    ):
        """
        åˆå§‹åŒ–èª¿åº¦å™¨

        åƒæ•¸ï¼š
            config: çˆ¬èŸ²é…ç½®
            telegram_app: Telegram Application å¯¦ä¾‹ï¼ˆç”¨æ–¼ç™¼é€é€šçŸ¥ï¼‰
        """
        self.config = config
        self.telegram_app = telegram_app
        self.crawler = NewsCrawler(config)
        self.scheduler = AsyncIOScheduler()

        logger.info("çˆ¬èŸ²èª¿åº¦å™¨åˆå§‹åŒ–å®Œæˆ")

    async def _crawl_and_notify(self):
        """
        çˆ¬å–æ–°èä¸¦ç™¼é€ Telegram é€šçŸ¥
        """
        try:
            # åŸ·è¡Œçˆ¬å–
            saved_news = await self.crawler.crawl()

            # ç™¼é€ Telegram é€šçŸ¥
            if saved_news and self.telegram_app and self.config.telegram_notify_groups:
                await self._send_telegram_notifications(saved_news)

        except Exception as e:
            logger.exception(f"çˆ¬èŸ²åŸ·è¡Œå¤±æ•—ï¼š{e}")

    async def _send_telegram_notifications(self, saved_news: list):
        """
        ç™¼é€ Telegram é€šçŸ¥

        åƒæ•¸ï¼š
            saved_news: å·²ä¿å­˜çš„æ–°èåˆ—è¡¨
        """
        for news in saved_news:
            message = (
                f"ğŸ“° **{news['commodity']} å•†å“æ–°è** (ID: {news['news_id']})\n\n"
                f"{news['text']}\n\n"
                f"â° {news.get('time', 'N/A')}"
            )

            # ç™¼é€åˆ°æ‰€æœ‰é…ç½®çš„ç¾¤çµ„
            for group_id in self.config.telegram_notify_groups:
                try:
                    await self.telegram_app.bot.send_message(
                        chat_id=group_id,
                        text=message,
                        parse_mode='Markdown'
                    )
                    logger.info(f"å·²ç™¼é€é€šçŸ¥åˆ°ç¾¤çµ„ {group_id}")

                except Exception as e:
                    logger.error(f"ç™¼é€é€šçŸ¥åˆ°ç¾¤çµ„ {group_id} å¤±æ•—ï¼š{e}")

    def start(self):
        """
        å•Ÿå‹•å®šæ™‚ä»»å‹™
        """
        if not self.config.enabled:
            logger.info("çˆ¬èŸ²å·²åœç”¨ï¼Œä¸å•Ÿå‹•å®šæ™‚ä»»å‹™")
            return

        # è¨ˆç®— jitterï¼ˆ5% éš¨æ©Ÿæ€§ï¼‰
        jitter = self.config.interval_jitter_seconds

        # æ–°å¢ä»»å‹™
        self.scheduler.add_job(
            self._crawl_and_notify,
            trigger=IntervalTrigger(
                minutes=self.config.crawl_interval_minutes,
                jitter=jitter
            ),
            id='news_crawler',
            name='å•†å“æ–°èçˆ¬èŸ²',
            replace_existing=True
        )

        # å•Ÿå‹•èª¿åº¦å™¨
        self.scheduler.start()

        logger.info(
            f"çˆ¬èŸ²å®šæ™‚ä»»å‹™å·²å•Ÿå‹•ï¼šæ¯ {self.config.crawl_interval_minutes} åˆ†é˜ "
            f"(Â±{jitter} ç§’) åŸ·è¡Œä¸€æ¬¡"
        )

    def stop(self):
        """
        åœæ­¢å®šæ™‚ä»»å‹™
        """
        if self.scheduler.running:
            self.scheduler.shutdown(wait=False)
            logger.info("çˆ¬èŸ²å®šæ™‚ä»»å‹™å·²åœæ­¢")
```

#### 3.3 æ•´åˆåˆ° Telegram Bot

**ä¿®æ”¹æª”æ¡ˆ**: `src/bot/telegram_bot.py`

**ä¿®æ”¹ä½ç½®**: `_post_init()` æ–¹æ³•ï¼ˆç¬¬ 81-95 è¡Œï¼‰

```python
from src.crawler.config import CrawlerConfig
from src.crawler.scheduler import CrawlerScheduler


class TelegramBot:
    def __init__(self, config: BotConfig):
        # ... åŸæœ‰ç¨‹å¼ç¢¼ ...

        # æ–°å¢ï¼šåˆå§‹åŒ–çˆ¬èŸ²èª¿åº¦å™¨
        crawler_config = CrawlerConfig.from_env()
        self.crawler_scheduler = CrawlerScheduler(
            config=crawler_config,
            telegram_app=self.application
        )

    async def _post_init(self, application: Application):
        """
        åˆå§‹åŒ–å¾Œå›èª¿

        åœ¨ Bot å•Ÿå‹•å¾ŒåŸ·è¡Œçš„åˆå§‹åŒ–ä»»å‹™ã€‚
        """
        logger.info("Bot å•Ÿå‹•å¾Œåˆå§‹åŒ–...")

        # å–å¾— Bot è³‡è¨Š
        bot = await application.bot.get_me()
        logger.info(f"Bot ç”¨æˆ¶åï¼š@{bot.username}")
        logger.info(f"Bot IDï¼š{bot.id}")

        # ç™¼é€é–‹å¼µè¨Šæ¯åˆ°æ‰€æœ‰é…ç½®çš„ç¾¤çµ„
        await self._send_startup_message(application)

        # æ–°å¢ï¼šå•Ÿå‹•çˆ¬èŸ²å®šæ™‚ä»»å‹™
        self.crawler_scheduler.start()
        logger.info("çˆ¬èŸ²å®šæ™‚ä»»å‹™å·²æ•´åˆåˆ° Bot ç”Ÿå‘½é€±æœŸ")

    async def _post_shutdown(self, application: Application):
        """
        é—œé–‰å¾Œå›èª¿

        åœ¨ Bot é—œé–‰å‰åŸ·è¡Œçš„æ¸…ç†ä»»å‹™ã€‚
        """
        logger.info("Bot æ­£åœ¨é—œé–‰...")

        # æ–°å¢ï¼šåœæ­¢çˆ¬èŸ²å®šæ™‚ä»»å‹™
        self.crawler_scheduler.stop()
        logger.info("çˆ¬èŸ²å®šæ™‚ä»»å‹™å·²åœæ­¢")
```

---

### 4. ç’°å¢ƒè®Šæ•¸é…ç½®

**ä¿®æ”¹æª”æ¡ˆ**: `.env.example`

**æ–°å¢å…§å®¹**ï¼š

```bash
# ============================================================================
# å•†å“æ–°èçˆ¬èŸ²è¨­å®š
# ============================================================================

# æ˜¯å¦å•Ÿç”¨çˆ¬èŸ²ï¼ˆå¯é¸ï¼Œé è¨­ç‚º trueï¼‰
CRAWLER_ENABLED=true

# ç›®æ¨™ç¶²ç«™ URLï¼ˆå¯é¸ï¼Œé è¨­ç‚º tradingeconomics.comï¼‰
CRAWLER_TARGET_URL=https://tradingeconomics.com/stream?c=commodity

# çˆ¬å–é–“éš”ï¼ˆåˆ†é˜ï¼Œå¯é¸ï¼Œé è¨­ç‚º 5ï¼‰
CRAWLER_INTERVAL_MINUTES=5

# é–“éš”éš¨æ©ŸåŒ–ç¯„åœï¼ˆç§’ï¼Œå¯é¸ï¼Œé è¨­ç‚º 15 ç§’ï¼Œå³ 5% éš¨æ©Ÿæ€§ï¼‰
CRAWLER_JITTER_SECONDS=15

# markets ç›®éŒ„è·¯å¾‘ï¼ˆå¯é¸ï¼Œé è¨­ç‚º 'markets'ï¼‰
MARKETS_DIR=markets

# è¦é€šçŸ¥çš„ Telegram ç¾¤çµ„ IDï¼ˆå¯é¸ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼‰
# è‹¥ä¸è¨­å®šå‰‡åªä¿å­˜åˆ°æª”æ¡ˆï¼Œä¸ç™¼é€é€šçŸ¥
# å¯ä½¿ç”¨ TELEGRAM_GROUP_IDS çš„å€¼
CRAWLER_NOTIFY_GROUPS=-1001234567890
```

---

### 5. éœ€è¦æ–°å¢çš„ä¾è³´å¥—ä»¶

**ä¿®æ”¹æª”æ¡ˆ**: `requirements.txt`

**æ–°å¢å…§å®¹**ï¼š

```txt
# æ–°èçˆ¬èŸ²ç›¸é—œ
httpx>=0.25.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
APScheduler>=3.10.0
```

**å®‰è£æŒ‡ä»¤**ï¼š

```bash
pip install httpx beautifulsoup4 lxml APScheduler
```

---

### 6. å¯¦ç¾æµç¨‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Telegram Bot å•Ÿå‹•                          â”‚
â”‚                  (run_bot.py)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TelegramBot._post_init()                                   â”‚
â”‚  - åˆå§‹åŒ– Bot                                               â”‚
â”‚  - ç™¼é€é–‹å¼µè¨Šæ¯                                             â”‚
â”‚  - å•Ÿå‹•çˆ¬èŸ²èª¿åº¦å™¨ (CrawlerScheduler.start())                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  APScheduler å®šæ™‚è§¸ç™¼ (æ¯ 5 åˆ†é˜ Â± 15 ç§’)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CrawlerScheduler._crawl_and_notify()                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NewsCrawler.crawl()                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. fetch_page()                                       â”‚  â”‚
â”‚  â”‚    - éš¨æ©Ÿå»¶é² 0.5-2 ç§’                                â”‚  â”‚
â”‚  â”‚    - éš¨æ©Ÿ User-Agent                                  â”‚  â”‚
â”‚  â”‚    - å½è£ Headers                                     â”‚  â”‚
â”‚  â”‚    - httpx.AsyncClient.get()                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 2. parse_news()                                       â”‚  â”‚
â”‚  â”‚    - BeautifulSoup è§£æ HTML                          â”‚  â”‚
â”‚  â”‚    - æå–æ–°èæ¨™é¡Œã€å…§å®¹ã€æ™‚é–“                         â”‚  â”‚
â”‚  â”‚    - è¿”å›æ–°èåˆ—è¡¨                                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 3. process_and_save()                                 â”‚  â”‚
â”‚  â”‚    For each news:                                     â”‚  â”‚
â”‚  â”‚    â”œâ”€ CommodityMapper.extract_commodity()             â”‚  â”‚
â”‚  â”‚    â”‚  (æå–å•†å“åç¨±)                                  â”‚  â”‚
â”‚  â”‚    â”œâ”€ NewsStorage.check_duplicate()                   â”‚  â”‚
â”‚  â”‚    â”‚  (æª¢æŸ¥é‡è¤‡)                                      â”‚  â”‚
â”‚  â”‚    â””â”€ NewsStorage.save_news()                         â”‚  â”‚
â”‚  â”‚       (ä¿å­˜åˆ° markets/<å•†å“>/yyyymmdd.txt)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CrawlerScheduler._send_telegram_notifications()            â”‚
â”‚  - éæ­·å·²ä¿å­˜çš„æ–°è                                         â”‚
â”‚  - æ ¼å¼åŒ–è¨Šæ¯                                               â”‚
â”‚  - application.bot.send_message() ç™¼é€åˆ°ç¾¤çµ„                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 7. æ½›åœ¨çš„æŠ€è¡“æŒ‘æˆ°å’Œè§£æ±ºæ–¹æ¡ˆ

#### 7.1 ç¶²ç«™çµæ§‹è®ŠåŒ–

**æŒ‘æˆ°**ï¼štradingeconomics.com çš„ HTML çµæ§‹å¯èƒ½æ”¹è®Šï¼Œå°è‡´è§£æå¤±æ•—ã€‚

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨å¤šå€‹ CSS é¸æ“‡å™¨å‚™é¸æ–¹æ¡ˆ
2. å¢åŠ éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„
3. å®šæœŸç›£æ§çˆ¬å–æˆåŠŸç‡
4. è€ƒæ…®ä½¿ç”¨ APIï¼ˆè‹¥ç¶²ç«™æä¾›ï¼‰

**ç¯„ä¾‹ç¨‹å¼ç¢¼**ï¼š
```python
def parse_news_safe(self, html: str) -> List[Dict]:
    """å®¹éŒ¯çš„æ–°èè§£æ"""
    soup = BeautifulSoup(html, 'lxml')

    # å˜—è©¦å¤šç¨®é¸æ“‡å™¨
    selectors = [
        'div.stream-item',
        'article.news-item',
        'div.commodity-news'
    ]

    for selector in selectors:
        items = soup.select(selector)
        if items:
            logger.debug(f"ä½¿ç”¨é¸æ“‡å™¨ï¼š{selector}")
            return self._parse_items(items)

    logger.warning("æ‰€æœ‰é¸æ“‡å™¨éƒ½ç„¡æ³•åŒ¹é…")
    return []
```

#### 7.2 IP å°é–é¢¨éšª

**æŒ‘æˆ°**ï¼šé »ç¹è«‹æ±‚å¯èƒ½å°è‡´ IP è¢«å°é–ã€‚

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
1. æ§åˆ¶è«‹æ±‚é »ç‡ï¼ˆ5 åˆ†é˜å·²è¶³å¤ å®‰å…¨ï¼‰
2. ä½¿ç”¨éš¨æ©Ÿå»¶é²å’Œ jitter
3. è¼ªæ› User-Agent
4. è‹¥éœ€è¦ï¼Œæ•´åˆä»£ç† IP æ± 

**ç¯„ä¾‹ç¨‹å¼ç¢¼**ï¼š
```python
# é…ç½®ä»£ç†ï¼ˆé¸ç”¨ï¼‰
async def fetch_with_proxy(self, url: str, proxy: str = None):
    async with httpx.AsyncClient(proxies=proxy) as client:
        return await client.get(url, ...)
```

#### 7.3 æ–°èé‡è¤‡éæ¿¾

**æŒ‘æˆ°**ï¼šåŒä¸€å‰‡æ–°èå¯èƒ½åœ¨ä¸åŒæ™‚é–“è¢«æŠ“åˆ°ã€‚

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
1. åŸºæ–¼å…§å®¹çš„å»é‡ï¼ˆç›®å‰å¯¦ç¾ï¼‰
2. å„²å­˜æ–°è hashï¼ˆMD5/SHA256ï¼‰
3. ä½¿ç”¨è³‡æ–™åº«ï¼ˆSQLiteï¼‰å„²å­˜å·²æŠ“å–çš„æ–°è ID

**å„ªåŒ–ç¯„ä¾‹**ï¼š
```python
import hashlib

def get_news_hash(self, text: str) -> str:
    """è¨ˆç®—æ–°è hash"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def check_duplicate_by_hash(self, news_hash: str) -> bool:
    """ä½¿ç”¨ hash æª¢æŸ¥é‡è¤‡"""
    # æŸ¥è©¢è³‡æ–™åº«æˆ–æª”æ¡ˆ
    pass
```

#### 7.4 ä½µç™¼å¯«å…¥è¡çª

**æŒ‘æˆ°**ï¼šè‹¥å¤šå€‹çˆ¬èŸ²å¯¦ä¾‹åŒæ™‚é‹è¡Œï¼Œå¯èƒ½å°è‡´æª”æ¡ˆå¯«å…¥è¡çªã€‚

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨æª”æ¡ˆé–ï¼ˆfcntl æˆ– msvcrtï¼‰
2. ç¢ºä¿åªæœ‰ä¸€å€‹ Bot å¯¦ä¾‹é‹è¡Œ
3. ä½¿ç”¨è³‡æ–™åº«æ›¿ä»£æª”æ¡ˆå„²å­˜

**Windows æª”æ¡ˆé–ç¯„ä¾‹**ï¼š
```python
import msvcrt

def save_with_lock(self, file_path: Path, content: str):
    """Windows æª”æ¡ˆé–"""
    with open(file_path, 'a', encoding='utf-8') as f:
        msvcrt.locking(f.fileno(), msvcrt.LK_LOCK, 1024)
        f.write(content)
        msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1024)
```

#### 7.5 å•†å“åç¨±åŒ¹é…æº–ç¢ºæ€§

**æŒ‘æˆ°**ï¼šæ–°èä¸­çš„å•†å“åç¨±å¯èƒ½æœ‰å¤šç¨®è¡¨é”æ–¹å¼ã€‚

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
1. æ“´å……å•†å“æ˜ å°„è¡¨ï¼ˆåŒ…å«åˆ¥åã€ç°¡ç¨±ï¼‰
2. ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æˆ– NLPï¼ˆå¦‚æœ‰éœ€è¦ï¼‰
3. è¨˜éŒ„æœªåŒ¹é…çš„æ–°èï¼Œæ‰‹å‹•åˆ†æ

**å„ªåŒ–ç¯„ä¾‹**ï¼š
```python
import re

COMMODITY_PATTERNS = {
    'Gold': [r'\bgold\b', r'\bxau\b', r'\bgc\b'],
    'Silver': [r'\bsilver\b', r'\bxag\b', r'\bsi\b'],
}

def extract_commodity_regex(self, text: str) -> Optional[str]:
    """ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åŒ¹é…"""
    for commodity, patterns in COMMODITY_PATTERNS.items():
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return commodity
    return None
```

---

### 8. æ¸¬è©¦è¨ˆç•«

#### 8.1 å–®å…ƒæ¸¬è©¦

**æ¸¬è©¦æª”æ¡ˆ**: `tests/test_crawler/`

```python
# tests/test_crawler/test_commodity_mapper.py
import pytest
from src.crawler.commodity_mapper import CommodityMapper

def test_extract_gold():
    mapper = CommodityMapper()
    assert mapper.extract_commodity("Gold prices surge") == "Gold"

def test_extract_no_match():
    mapper = CommodityMapper()
    assert mapper.extract_commodity("Some random news") is None

# tests/test_crawler/test_news_storage.py
from src.crawler.news_storage import NewsStorage

def test_save_news(tmp_path):
    storage = NewsStorage(markets_dir=str(tmp_path))
    success, news_id = storage.save_news('Gold', 'Test news')
    assert success
    assert news_id == 1
```

#### 8.2 æ•´åˆæ¸¬è©¦

```python
# tests/test_crawler/test_integration.py
import pytest
from src.crawler.news_crawler import NewsCrawler
from src.crawler.config import CrawlerConfig

@pytest.mark.asyncio
async def test_full_crawl():
    config = CrawlerConfig(
        target_url='https://tradingeconomics.com/stream?c=commodity',
        crawl_interval_minutes=5,
        interval_jitter_seconds=15,
        markets_dir='markets',
        enabled=True,
        telegram_notify_groups=[]
    )

    crawler = NewsCrawler(config)
    saved_news = await crawler.crawl()

    # æª¢æŸ¥æ˜¯å¦æœ‰æ–°èè¢«ä¿å­˜
    assert isinstance(saved_news, list)
```

#### 8.3 æ‰‹å‹•æ¸¬è©¦

1. **å•Ÿå‹• Bot ä¸¦é©—è­‰çˆ¬èŸ²å•Ÿå‹•**ï¼š
   ```bash
   python scripts/run_bot.py
   # æª¢æŸ¥æ—¥èªŒï¼šã€Œçˆ¬èŸ²å®šæ™‚ä»»å‹™å·²å•Ÿå‹•ã€
   ```

2. **æª¢æŸ¥ markets/ ç›®éŒ„**ï¼š
   ```bash
   ls -la markets/Gold/
   # æ‡‰è©²çœ‹åˆ° yyyymmdd.txt æª”æ¡ˆ
   ```

3. **æª¢æŸ¥ Telegram é€šçŸ¥**ï¼š
   - åœ¨ç¾¤çµ„ä¸­ç¢ºèªæ˜¯å¦æ”¶åˆ°æ–°èé€šçŸ¥

---

### 9. å¯¦ä½œæ­¥é©Ÿå»ºè­°

1. **éšæ®µä¸€ï¼šåŸºç¤æ¶æ§‹**ï¼ˆ1-2 å°æ™‚ï¼‰
   - å»ºç«‹ `src/crawler/` ç›®éŒ„
   - å¯¦ç¾ `config.py`
   - å¯¦ç¾ `commodity_mapper.py`
   - å¯¦ç¾ `news_storage.py`
   - æ’°å¯«å–®å…ƒæ¸¬è©¦

2. **éšæ®µäºŒï¼šçˆ¬èŸ²æ ¸å¿ƒ**ï¼ˆ1-2 å°æ™‚ï¼‰
   - å¯¦ç¾ `news_crawler.py`
   - åˆ†æ tradingeconomics.com HTML çµæ§‹
   - èª¿æ•´ CSS é¸æ“‡å™¨
   - æ¸¬è©¦ç¶²é æŠ“å–å’Œè§£æ

3. **éšæ®µä¸‰ï¼šå®šæ™‚ä»»å‹™**ï¼ˆ30 åˆ†é˜ï¼‰
   - å¯¦ç¾ `scheduler.py`
   - æ•´åˆåˆ° `telegram_bot.py`
   - æ¸¬è©¦å®šæ™‚è§¸ç™¼

4. **éšæ®µå››ï¼šTelegram é€šçŸ¥**ï¼ˆ30 åˆ†é˜ï¼‰
   - å¯¦ç¾é€šçŸ¥ç™¼é€é‚è¼¯
   - æ¸¬è©¦è¨Šæ¯æ ¼å¼
   - èª¿æ•´è¨Šæ¯å…§å®¹

5. **éšæ®µäº”ï¼šæ¸¬è©¦èˆ‡å„ªåŒ–**ï¼ˆ1 å°æ™‚ï¼‰
   - æ•´åˆæ¸¬è©¦
   - è™•ç†é‚Šç•Œæƒ…æ³
   - å„ªåŒ–æ•ˆèƒ½å’ŒéŒ¯èª¤è™•ç†
   - æ’°å¯«æ–‡æª”

---

### 10. æœªä¾†æ“´å……å»ºè­°

1. **è³‡æ–™åº«å„²å­˜**ï¼š
   - ä½¿ç”¨ SQLite å„²å­˜æ–°èï¼Œæ›¿ä»£ç´”æ–‡å­—æª”æ¡ˆ
   - æ”¯æ´æ›´è¤‡é›œçš„æŸ¥è©¢å’Œå»é‡é‚è¼¯

2. **å¤šèªè¨€æ”¯æ´**ï¼š
   - ä½¿ç”¨ç¿»è­¯ API å°‡è‹±æ–‡æ–°èç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡
   - æä¾›é›™èªé€šçŸ¥

3. **æƒ…æ„Ÿåˆ†æ**ï¼š
   - ä½¿ç”¨ NLP åˆ†ææ–°èæƒ…æ„Ÿï¼ˆæ­£é¢/è² é¢ï¼‰
   - æ¨™è¨˜é‡è¦æ–°èï¼ˆå¦‚åƒ¹æ ¼å¤§å¹…æ³¢å‹•ï¼‰

4. **Web Dashboard**ï¼š
   - æä¾› Web ä»‹é¢æŸ¥çœ‹æ­·å²æ–°è
   - å¯è¦–åŒ–æ–°èè¶¨å‹¢

5. **å¤šä¾†æºæ•´åˆ**ï¼š
   - æ•´åˆå¤šå€‹æ–°èä¾†æºï¼ˆReuters, Bloomberg ç­‰ï¼‰
   - èšåˆä¸¦å»é‡

---

## ç¨‹å¼ç¢¼ç¯„ä¾‹ç¸½çµ

### æœ€å°å¯è¡Œå¯¦ç¾ï¼ˆMVPï¼‰

**æª”æ¡ˆ**: `examples/simple_crawler.py`

```python
#!/usr/bin/env python3
"""
ç°¡åŒ–ç‰ˆå•†å“æ–°èçˆ¬èŸ²ç¯„ä¾‹

æ­¤ç¯„ä¾‹å±•ç¤ºæœ€å°å¯è¡Œå¯¦ç¾ï¼ˆç„¡ Telegram æ•´åˆï¼‰ã€‚
"""

import asyncio
import httpx
from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime


async def simple_crawl():
    """ç°¡å–®çš„æ–°èçˆ¬èŸ²ç¯„ä¾‹"""

    # 1. æŠ“å–ç¶²é 
    url = 'https://tradingeconomics.com/stream?c=commodity'
    async with httpx.AsyncClient() as client:
        response = await client.get(url, timeout=30.0)
        html = response.text

    # 2. è§£ææ–°è
    soup = BeautifulSoup(html, 'lxml')
    items = soup.select('div.stream-item')  # éœ€æ ¹æ“šå¯¦éš›èª¿æ•´

    # 3. å„²å­˜æ–°è
    markets_dir = Path('markets')
    date_str = datetime.now().strftime('%Y%m%d')

    for item in items:
        title = item.select_one('h3').get_text(strip=True)

        # ç°¡å–®çš„å•†å“åŒ¹é…
        if 'gold' in title.lower():
            commodity = 'Gold'
        elif 'silver' in title.lower():
            commodity = 'Silver'
        else:
            continue

        # ä¿å­˜åˆ°æª”æ¡ˆ
        file_path = markets_dir / commodity / f"{date_str}.txt"
        file_path.parent.mkdir(parents=True, exist_ok=True)

        with open(file_path, 'a', encoding='utf-8') as f:
            f.write(f"{title}\n")
            f.write("-" * 80 + "\n")

        print(f"å·²ä¿å­˜ï¼š{commodity} - {title}")


if __name__ == '__main__':
    asyncio.run(simple_crawl())
```

---

## é™„éŒ„

### A. tradingeconomics.com HTML çµæ§‹åˆ†æ

**æ³¨æ„**ï¼šå¯¦éš›å¯¦ç¾æ™‚éœ€è¦æ‰‹å‹•è¨ªå•ç¶²ç«™ä¸¦æª¢æŸ¥ HTML çµæ§‹ã€‚

**å»ºè­°æ­¥é©Ÿ**ï¼š
1. ä½¿ç”¨ç€è¦½å™¨é–‹ç™¼è€…å·¥å…·ï¼ˆF12ï¼‰
2. æ‰¾åˆ°æ–°èåˆ—è¡¨çš„å®¹å™¨å…ƒç´ 
3. è¨˜éŒ„ CSS é¸æ“‡å™¨æˆ– XPath
4. ç¢ºèªæ–°èæ¨™é¡Œã€å…§å®¹ã€æ™‚é–“çš„å…ƒç´ ä½ç½®

**ç¯„ä¾‹åˆ†æ**ï¼ˆéœ€æ ¹æ“šå¯¦éš›èª¿æ•´ï¼‰ï¼š
```html
<!-- å‡è¨­çš„ HTML çµæ§‹ -->
<div class="stream-container">
    <div class="stream-item" data-id="12345">
        <h3 class="stream-title">Gold prices rise on inflation fears</h3>
        <p class="stream-content">Gold futures climbed...</p>
        <time datetime="2026-01-02T10:30:00Z">2 hours ago</time>
    </div>
    <!-- æ›´å¤šæ–°è... -->
</div>
```

**å°æ‡‰çš„ CSS é¸æ“‡å™¨**ï¼š
```python
items = soup.select('div.stream-item')
title = item.select_one('h3.stream-title')
content = item.select_one('p.stream-content')
time = item.select_one('time')
```

### B. åƒè€ƒè³‡æº

- **httpx æ–‡æª”**: https://www.python-httpx.org/
- **BeautifulSoup æ–‡æª”**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **APScheduler æ–‡æª”**: https://apscheduler.readthedocs.io/
- **python-telegram-bot æ–‡æª”**: https://docs.python-telegram-bot.org/

---

**ç ”ç©¶å®Œæˆæ™‚é–“**: 2026-01-02
**é ä¼°å¯¦ä½œæ™‚é–“**: 4-6 å°æ™‚ï¼ˆåŒ…å«æ¸¬è©¦å’Œé™¤éŒ¯ï¼‰
**å»ºè­°å„ªå…ˆç´š**: ğŸ”´ é«˜ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
