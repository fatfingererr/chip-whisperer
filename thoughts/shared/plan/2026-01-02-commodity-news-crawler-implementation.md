# å•†å“æ–°èçˆ¬èŸ²åŠŸèƒ½å¯¦ä½œè¨ˆç•«

## æ¦‚è¿°

æ ¹æ“šç ”ç©¶å ±å‘Š `thoughts/shared/research/2026-01-02-commodity-news-crawler-research.md`ï¼Œæœ¬è¨ˆç•«æ—¨åœ¨å¯¦ä½œä¸€å€‹è‡ªå‹•åŒ–çš„å•†å“æ–°èçˆ¬èŸ²ç³»çµ±ï¼Œæ¯ 5 åˆ†é˜ï¼ˆÂ±5% éš¨æ©Ÿæ€§ï¼‰å¾ tradingeconomics.com æŠ“å–å•†å“ç›¸é—œæ–°èï¼Œä¸¦å°‡æ–°èå„²å­˜åˆ°å°æ‡‰çš„å•†å“ç›®éŒ„ä¸­ï¼ŒåŒæ™‚ç™¼é€é€šçŸ¥åˆ° Telegram ç¾¤çµ„ã€‚

## ç›®å‰ç‹€æ…‹åˆ†æ

### å·²å­˜åœ¨çš„å…ƒä»¶

1. **Telegram Bot æ¶æ§‹** (`src/bot/telegram_bot.py`)
   - æ”¯æ´ async/await æ¨¡å¼
   - æä¾› `_post_init()` å’Œ `_post_shutdown()` ç”Ÿå‘½é€±æœŸé‰¤å­
   - å·²æœ‰ç¾¤çµ„è¨Šæ¯ç™¼é€æ©Ÿåˆ¶
   - Application å¯¦ä¾‹å¯å…±äº«çµ¦å…¶ä»–æ¨¡çµ„

2. **é…ç½®ç®¡ç†ç³»çµ±** (`src/bot/config.py`)
   - BotConfig è³‡æ–™é¡åˆ¥
   - ç’°å¢ƒè®Šæ•¸è¼‰å…¥æ©Ÿåˆ¶
   - å¯æ“´å……æ–°å¢çˆ¬èŸ²é…ç½®

3. **æ—¥èªŒç³»çµ±** (`scripts/run_bot.py`)
   - Loguru æ—¥èªŒåº«
   - æ§åˆ¶å° + æª”æ¡ˆè¼¸å‡º
   - æ¯æ—¥è¼ªæ›ï¼Œä¿ç•™ 30 å¤©

4. **Markets ç›®éŒ„çµæ§‹** (`markets/`)
   - ç¾æœ‰ 20 å€‹å•†å“ç›®éŒ„ï¼ˆGold, Silver, Brent, Wti, Bitcoin, Ethereum, Solana ç­‰ï¼‰
   - ç›®å‰ç›®éŒ„ç‚ºç©ºï¼Œå¾…å¡«å……æ–°èè³‡æ–™

### ç¼ºå°‘çš„å…ƒä»¶

1. **çˆ¬èŸ²æ¨¡çµ„** (`src/crawler/`)
   - æ–°èçˆ¬èŸ²æ ¸å¿ƒ
   - å•†å“åç¨±æ˜ å°„å™¨
   - æ–°èå„²å­˜ç®¡ç†
   - å®šæ™‚ä»»å‹™èª¿åº¦å™¨
   - çˆ¬èŸ²é…ç½®

2. **ä¾è³´å¥—ä»¶**
   - httpxï¼ˆHTTP å®¢æˆ¶ç«¯ï¼‰
   - beautifulsoup4ï¼ˆHTML è§£æï¼‰
   - lxmlï¼ˆHTML parserï¼‰
   - APSchedulerï¼ˆå®šæ™‚ä»»å‹™ï¼‰

3. **ç’°å¢ƒè®Šæ•¸é…ç½®**
   - çˆ¬èŸ²å•Ÿç”¨é–‹é—œ
   - ç›®æ¨™ URL
   - çˆ¬å–é–“éš”è¨­å®š
   - é€šçŸ¥ç¾¤çµ„ ID

## æœŸæœ›çš„æœ€çµ‚ç‹€æ…‹

### åŠŸèƒ½æ€§é©—è­‰

1. **å®šæ™‚çˆ¬å–é‹ä½œä¸­**
   - Bot å•Ÿå‹•å¾Œè‡ªå‹•å•Ÿå‹•çˆ¬èŸ²å®šæ™‚ä»»å‹™
   - æ¯ 5 åˆ†é˜åŸ·è¡Œä¸€æ¬¡ï¼ˆÂ±15 ç§’éš¨æ©Ÿæ€§ï¼‰
   - æ—¥èªŒé¡¯ç¤ºçˆ¬å–é–‹å§‹å’ŒçµæŸè¨Šæ¯

2. **æ–°èæ­£ç¢ºå„²å­˜**
   - æ–°èä¿å­˜åˆ° `markets/<å•†å“>/yyyymmdd.txt`
   - æ¯å‰‡æ–°èæœ‰éå¢ IDï¼ˆæ ¼å¼ï¼š`[1] æ–°èå…§å®¹`ï¼‰
   - åŒä¸€å¤©çš„æ–°èè¿½åŠ åˆ°åŒä¸€æª”æ¡ˆ
   - é‡è¤‡æ–°èè¢«éæ¿¾ä¸ä¿å­˜

3. **Telegram é€šçŸ¥æ­£å¸¸**
   - æ–°æŠ“å–çš„æ–°èç«‹å³ç™¼é€åˆ°é…ç½®çš„ç¾¤çµ„
   - è¨Šæ¯æ ¼å¼æ¸…æ™°ï¼ˆå•†å“åç¨±ã€IDã€å…§å®¹ã€æ™‚é–“ï¼‰
   - ç™¼é€å¤±æ•—æœ‰éŒ¯èª¤æ—¥èªŒè¨˜éŒ„

4. **é˜²çˆ¬èŸ²æ©Ÿåˆ¶ç”Ÿæ•ˆ**
   - è«‹æ±‚å‰æœ‰ 0.5-2 ç§’éš¨æ©Ÿå»¶é²
   - User-Agent éš¨æ©Ÿè¼ªæ›
   - Headers å½è£å®Œæ•´

### éåŠŸèƒ½æ€§é©—è­‰

1. **æ•ˆèƒ½ç©©å®š**
   - ä¸å½±éŸ¿ Telegram Bot ä¸»åŠŸèƒ½
   - è¨˜æ†¶é«”å ç”¨ç©©å®šï¼ˆç„¡è¨˜æ†¶é«”æ´©æ¼ï¼‰
   - çˆ¬å–å¤±æ•—ä¸å½±éŸ¿ Bot é‹è¡Œ

2. **å¯ç¶­è­·æ€§**
   - ç¨‹å¼ç¢¼çµæ§‹æ¸…æ™°ï¼Œè·è²¬åˆ†é›¢
   - æœ‰å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œæ—¥èªŒ
   - é…ç½®å¯é€éç’°å¢ƒè®Šæ•¸éˆæ´»èª¿æ•´

3. **å¯æ“´å±•æ€§**
   - å•†å“æ˜ å°„è¡¨æ˜“æ–¼æ–°å¢
   - å¯è¼•é¬†èª¿æ•´çˆ¬å–é–“éš”
   - å¯åœç”¨çˆ¬èŸ²åŠŸèƒ½ï¼ˆCRAWLER_ENABLED=falseï¼‰

## æˆ‘å€‘ä¸åšçš„äº‹æƒ…

ç‚ºé¿å…ç¯„åœè†¨è„¹ï¼Œä»¥ä¸‹åŠŸèƒ½æ˜ç¢ºæ’é™¤åœ¨æœ¬æ¬¡å¯¦ä½œä¹‹å¤–ï¼š

1. âŒ **å¤šèªè¨€ç¿»è­¯**ï¼šä¸å°‡è‹±æ–‡æ–°èç¿»è­¯ç‚ºä¸­æ–‡
2. âŒ **æƒ…æ„Ÿåˆ†æ**ï¼šä¸åˆ†ææ–°èæƒ…æ„Ÿï¼ˆæ­£é¢/è² é¢ï¼‰
3. âŒ **Web Dashboard**ï¼šä¸æä¾›ç¶²é ä»‹é¢æŸ¥çœ‹æ–°è
4. âŒ **è³‡æ–™åº«å„²å­˜**ï¼šæœ¬éšæ®µä½¿ç”¨ç´”æ–‡å­—æª”æ¡ˆï¼Œä¸ä½¿ç”¨ SQLite
5. âŒ **å¤šä¾†æºæ•´åˆ**ï¼šåªçˆ¬å– tradingeconomics.comï¼Œä¸æ•´åˆå…¶ä»–ç¶²ç«™
6. âŒ **ä»£ç† IP æ± **ï¼šåˆæœŸä¸æ•´åˆä»£ç† IPï¼Œä¾è³´åŸºæœ¬é˜²çˆ¬ç­–ç•¥
7. âŒ **æ–°èåˆ†é¡**ï¼šä¸å°æ–°èé€²è¡Œè‡ªå‹•åˆ†é¡æˆ–æ¨™ç±¤
8. âŒ **æ­·å²è³‡æ–™å›å¡«**ï¼šä¸å›å¡«éå»çš„æ–°èè³‡æ–™

## å¯¦ä½œæ–¹æ³•

æ¡ç”¨åˆ†éšæ®µã€å¢é‡å¼çš„é–‹ç™¼æ–¹å¼ï¼Œæ¯å€‹éšæ®µéƒ½æœ‰æ˜ç¢ºçš„å¯é©—è­‰ç›®æ¨™ã€‚å„éšæ®µä¹‹é–“æœ‰æ¸…æ™°çš„ä¾è³´é—œä¿‚ï¼Œç¢ºä¿ç©©å›ºçš„åŸºç¤ä¸Šé€æ­¥å¢åŠ åŠŸèƒ½ã€‚

### é—œéµæŠ€è¡“æ±ºç­–

1. **HTTP å®¢æˆ¶ç«¯**ï¼šhttpxï¼ˆæ”¯æ´ async/awaitï¼Œèˆ‡ç¾æœ‰æ¶æ§‹å¥‘åˆï¼‰
2. **HTML è§£æ**ï¼šBeautifulSoup4 + lxmlï¼ˆæ˜“ç”¨ä¸”å®¹éŒ¯èƒ½åŠ›å¼·ï¼‰
3. **å®šæ™‚ä»»å‹™**ï¼šAPSchedulerï¼ˆæ”¯æ´ jitterï¼Œèˆ‡ asyncio æ•´åˆè‰¯å¥½ï¼‰
4. **æª”æ¡ˆé–**ï¼šWindows ä½¿ç”¨ try-except åŒ…è£ï¼ˆé¿å… fcntl ä¸ç›¸å®¹å•é¡Œï¼‰
5. **å»é‡ç­–ç•¥**ï¼šç°¡å–®çš„å­—ä¸²åŒ…å«æª¢æŸ¥ï¼ˆåˆæœŸæ–¹æ¡ˆï¼Œå¯å‡ç´šç‚º hashï¼‰

---

## éšæ®µä¸€ï¼šå‰ç½®æº–å‚™èˆ‡ç’°å¢ƒè¨­å®š

### æ¦‚è¿°

å»ºç«‹çˆ¬èŸ²æ¨¡çµ„çš„åŸºç¤æ¶æ§‹ï¼Œå®‰è£å¿…è¦ä¾è³´ï¼Œé…ç½®ç’°å¢ƒè®Šæ•¸ï¼Œç¢ºä¿å°ˆæ¡ˆå¯æ­£å¸¸å•Ÿå‹•ã€‚

### éœ€è¦å‰µå»º/ä¿®æ”¹çš„æª”æ¡ˆ

#### 1. å®‰è£ä¾è³´å¥—ä»¶

**æª”æ¡ˆ**ï¼š`requirements.txt`

**ä¿®æ”¹å…§å®¹**ï¼šåœ¨æª”æ¡ˆæœ«å°¾æ–°å¢

```txt
# æ–°èçˆ¬èŸ²ç›¸é—œ
httpx>=0.25.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
APScheduler>=3.10.0
```

**åŸ·è¡ŒæŒ‡ä»¤**ï¼š
```bash
pip install httpx beautifulsoup4 lxml APScheduler
```

#### 2. æ›´æ–°ç’°å¢ƒè®Šæ•¸ç¯„æœ¬

**æª”æ¡ˆ**ï¼š`.env.example`

**ä¿®æ”¹å…§å®¹**ï¼šåœ¨æª”æ¡ˆæœ«å°¾æ–°å¢

```bash
# ============================================================================
# å•†å“æ–°èçˆ¬èŸ²è¨­å®š
# ============================================================================

# æ˜¯å¦å•Ÿç”¨çˆ¬èŸ²ï¼ˆå¯é¸ï¼Œé è¨­ç‚º trueï¼‰
CRAWLER_ENABLED=true

# ç›®æ¨™ç¶²ç«™ URLï¼ˆå¯é¸ï¼Œé è¨­ç‚º tradingeconomics.comï¼‰
CRAWLER_TARGET_URL=https://tradingeconomics.com/stream?c=commodity

# çˆ¬å–é–“éš”ï¼ˆåˆ†é˜ï¼Œå¯é¸ï¼Œé è¨­ç‚º 5ï¼‰
CRAWLER_INTERVAL_MINUTES=5

# é–“éš”éš¨æ©ŸåŒ–ç¯„åœï¼ˆç§’ï¼Œå¯é¸ï¼Œé è¨­ç‚º 15 ç§’ï¼Œå³ 5% éš¨æ©Ÿæ€§ï¼‰
CRAWLER_JITTER_SECONDS=15

# markets ç›®éŒ„è·¯å¾‘ï¼ˆå¯é¸ï¼Œé è¨­ç‚º 'markets'ï¼‰
MARKETS_DIR=markets

# è¦é€šçŸ¥çš„ Telegram ç¾¤çµ„ IDï¼ˆå¯é¸ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼‰
# è‹¥ä¸è¨­å®šå‰‡åªä¿å­˜åˆ°æª”æ¡ˆï¼Œä¸ç™¼é€é€šçŸ¥
# å»ºè­°ä½¿ç”¨èˆ‡ TELEGRAM_GROUP_IDS ç›¸åŒçš„å€¼
CRAWLER_NOTIFY_GROUPS=
```

#### 3. æ›´æ–°å¯¦éš›ç’°å¢ƒè¨­å®š

**æª”æ¡ˆ**ï¼š`.env`ï¼ˆè«‹æ‰‹å‹•å‰µå»ºæˆ–ä¿®æ”¹ï¼‰

**ä¿®æ”¹å…§å®¹**ï¼šè¤‡è£½ `.env.example` çš„æ–°å¢å…§å®¹ï¼Œä¸¦å¡«å…¥å¯¦éš›å€¼

```bash
CRAWLER_ENABLED=true
CRAWLER_TARGET_URL=https://tradingeconomics.com/stream?c=commodity
CRAWLER_INTERVAL_MINUTES=5
CRAWLER_JITTER_SECONDS=15
MARKETS_DIR=markets
CRAWLER_NOTIFY_GROUPS=-1001234567890  # æ›¿æ›ç‚ºå¯¦éš›çš„ç¾¤çµ„ ID
```

#### 4. å»ºç«‹çˆ¬èŸ²æ¨¡çµ„ç›®éŒ„çµæ§‹

**åŸ·è¡ŒæŒ‡ä»¤**ï¼š
```bash
# å»ºç«‹ src/crawler ç›®éŒ„
mkdir -p src/crawler

# å»ºç«‹ __init__.pyï¼ˆæ¨¡çµ„æ¨™è¨˜æª”æ¡ˆï¼‰
touch src/crawler/__init__.py
```

### æˆåŠŸæ¨™æº–

#### è‡ªå‹•åŒ–é©—è­‰
- [ ] `pip list` é¡¯ç¤º httpx, beautifulsoup4, lxml, APScheduler å·²å®‰è£
- [ ] `.env.example` åŒ…å«çˆ¬èŸ²ç›¸é—œç’°å¢ƒè®Šæ•¸èªªæ˜
- [ ] `src/crawler/` ç›®éŒ„å­˜åœ¨ä¸”åŒ…å« `__init__.py`

#### æ‰‹å‹•é©—è­‰
- [ ] `.env` æª”æ¡ˆå·²æ›´æ–°ä¸¦å¡«å…¥å¯¦éš›è¨­å®šå€¼
- [ ] åŸ·è¡Œ `python scripts/run_bot.py` å¯æ­£å¸¸å•Ÿå‹•ï¼ˆä¸å ±éŒ¯ï¼‰

**å¯¦ä½œæç¤º**ï¼šæ­¤éšæ®µä¸æ¶‰åŠçˆ¬èŸ²é‚è¼¯ï¼Œåªæ˜¯æº–å‚™ç’°å¢ƒï¼Œæ‡‰è©²å¿«é€Ÿå®Œæˆï¼ˆ15-30 åˆ†é˜ï¼‰ã€‚

---

## éšæ®µäºŒï¼šåŸºç¤æ¶æ§‹æ¨¡çµ„

### æ¦‚è¿°

å¯¦ä½œçˆ¬èŸ²çš„åŸºç¤å…ƒä»¶ï¼šé…ç½®ç®¡ç†ã€å•†å“åç¨±æ˜ å°„ã€æ–°èå„²å­˜ç®¡ç†ã€‚é€™äº›æ˜¯çˆ¬èŸ²æ ¸å¿ƒçš„æ”¯æ’æ¨¡çµ„ã€‚

### éœ€è¦å‰µå»ºçš„æª”æ¡ˆ

#### 1. çˆ¬èŸ²é…ç½®æ¨¡çµ„

**æª”æ¡ˆ**ï¼š`src/crawler/config.py`

**åŠŸèƒ½**ï¼š
- å®šç¾© `CrawlerConfig` è³‡æ–™é¡åˆ¥
- å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥çˆ¬èŸ²é…ç½®
- é©—è­‰é…ç½®çš„æœ‰æ•ˆæ€§

**å®Œæ•´ç¨‹å¼ç¢¼**ï¼š

```python
"""
çˆ¬èŸ²é…ç½®æ¨¡çµ„

ç®¡ç†çˆ¬èŸ²ç›¸é—œçš„é…ç½®åƒæ•¸ã€‚
"""

from dataclasses import dataclass
from typing import List
import os
from dotenv import load_dotenv


@dataclass
class CrawlerConfig:
    """
    çˆ¬èŸ²é…ç½®è³‡æ–™é¡åˆ¥

    å±¬æ€§ï¼š
        target_url: ç›®æ¨™ç¶²ç«™ URL
        crawl_interval_minutes: çˆ¬å–é–“éš”ï¼ˆåˆ†é˜ï¼‰
        interval_jitter_seconds: é–“éš”éš¨æ©ŸåŒ–ç¯„åœï¼ˆç§’ï¼‰
        markets_dir: markets ç›®éŒ„è·¯å¾‘
        enabled: æ˜¯å¦å•Ÿç”¨çˆ¬èŸ²
        telegram_notify_groups: è¦é€šçŸ¥çš„ Telegram ç¾¤çµ„ ID åˆ—è¡¨
    """

    target_url: str
    crawl_interval_minutes: int
    interval_jitter_seconds: int
    markets_dir: str
    enabled: bool
    telegram_notify_groups: List[int]

    @classmethod
    def from_env(cls) -> 'CrawlerConfig':
        """
        å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥é…ç½®

        å›å‚³ï¼š
            CrawlerConfig å¯¦ä¾‹
        """
        load_dotenv()

        return cls(
            target_url=os.getenv(
                'CRAWLER_TARGET_URL',
                'https://tradingeconomics.com/stream?c=commodity'
            ),
            crawl_interval_minutes=int(os.getenv('CRAWLER_INTERVAL_MINUTES', '5')),
            interval_jitter_seconds=int(os.getenv('CRAWLER_JITTER_SECONDS', '15')),
            markets_dir=os.getenv('MARKETS_DIR', 'markets'),
            enabled=os.getenv('CRAWLER_ENABLED', 'true').lower() in ('true', '1', 'yes'),
            telegram_notify_groups=[
                int(gid.strip())
                for gid in os.getenv('CRAWLER_NOTIFY_GROUPS', '').split(',')
                if gid.strip()
            ]
        )
```

**æ¸¬è©¦æ–¹å¼**ï¼š

```python
# åœ¨ Python REPL ä¸­æ¸¬è©¦
from src.crawler.config import CrawlerConfig

config = CrawlerConfig.from_env()
print(f"å•Ÿç”¨ç‹€æ…‹: {config.enabled}")
print(f"ç›®æ¨™ URL: {config.target_url}")
print(f"çˆ¬å–é–“éš”: {config.crawl_interval_minutes} åˆ†é˜")
print(f"é€šçŸ¥ç¾¤çµ„: {config.telegram_notify_groups}")
```

#### 2. å•†å“åç¨±æ˜ å°„æ¨¡çµ„

**æª”æ¡ˆ**ï¼š`src/crawler/commodity_mapper.py`

**åŠŸèƒ½**ï¼š
- æä¾›æ–°èé—œéµå­—èˆ‡ markets ç›®éŒ„çš„æ˜ å°„é—œä¿‚
- å¾æ–°èæ–‡æœ¬ä¸­æå–å•†å“åç¨±
- é©—è­‰å•†å“ç›®éŒ„æ˜¯å¦å­˜åœ¨

**å®Œæ•´ç¨‹å¼ç¢¼**ï¼š

```python
"""
å•†å“åç¨±æ˜ å°„æ¨¡çµ„

æä¾›æ–°èä¸­çš„å•†å“åç¨±èˆ‡ markets/ ç›®éŒ„çš„æ˜ å°„é—œä¿‚ã€‚
"""

from typing import Optional
from pathlib import Path
from loguru import logger


class CommodityMapper:
    """
    å•†å“åç¨±æ˜ å°„å™¨

    è² è²¬å°‡æ–°èä¸­çš„å•†å“åç¨±æ˜ å°„åˆ° markets/ ç›®éŒ„åç¨±ã€‚
    """

    # å•†å“åç¨±æ˜ å°„è¡¨ï¼ˆæ–°èé—œéµå­— -> markets ç›®éŒ„åï¼‰
    COMMODITY_MAP = {
        # è²´é‡‘å±¬
        'gold': 'Gold',
        'silver': 'Silver',
        'platinum': 'Platinum',
        'palladium': 'Palladium',

        # èƒ½æº
        'crude oil': 'Wti',
        'wti': 'Wti',
        'brent': 'Brent',
        'oil': 'Wti',  # é è¨­ç‚º WTI

        # åŸºæœ¬é‡‘å±¬
        'copper': 'Copper',
        'aluminium': 'Aluminium',
        'aluminum': 'Aluminium',  # ç¾å¼æ‹¼æ³•
        'zinc': 'Zinc',
        'lead': 'Lead',

        # åŠ å¯†è²¨å¹£
        'bitcoin': 'Bitcoin',
        'btc': 'Bitcoin',
        'ethereum': 'Ethereum',
        'eth': 'Ethereum',
        'solana': 'Solana',
        'sol': 'Solana',

        # è¾²ç”¢å“
        'cocoa': 'Cocoa',
        'coffee': 'Coffee',
        'corn': 'Corn',
        'cotton': 'Cotton',
        'soybean': 'Sbean',
        'sugar': 'Sugar',
        'wheat': 'Wheat',
    }

    def __init__(self, markets_dir: str = 'markets'):
        """
        åˆå§‹åŒ–æ˜ å°„å™¨

        åƒæ•¸ï¼š
            markets_dir: markets ç›®éŒ„è·¯å¾‘
        """
        self.markets_dir = Path(markets_dir)
        self._load_available_commodities()

    def _load_available_commodities(self):
        """è¼‰å…¥ markets/ ç›®éŒ„ä¸‹å¯¦éš›å­˜åœ¨çš„å•†å“"""
        if not self.markets_dir.exists():
            logger.warning(f"markets ç›®éŒ„ä¸å­˜åœ¨ï¼š{self.markets_dir}")
            self.available_commodities = set()
            return

        # å–å¾—æ‰€æœ‰å­ç›®éŒ„åç¨±
        self.available_commodities = {
            d.name for d in self.markets_dir.iterdir()
            if d.is_dir() and not d.name.startswith('.')
        }

        logger.info(f"å·²è¼‰å…¥ {len(self.available_commodities)} å€‹å¯ç”¨å•†å“ç›®éŒ„")
        logger.debug(f"å¯ç”¨å•†å“ï¼š{sorted(self.available_commodities)}")

    def extract_commodity(self, news_text: str) -> Optional[str]:
        """
        å¾æ–°èæ–‡æœ¬ä¸­æå–å•†å“åç¨±

        åƒæ•¸ï¼š
            news_text: æ–°èæ–‡æœ¬ï¼ˆè‹±æ–‡ï¼‰

        å›å‚³ï¼š
            å•†å“ç›®éŒ„åç¨±ï¼ˆå¦‚ 'Gold'ï¼‰ï¼Œè‹¥ç„¡åŒ¹é…å‰‡å›å‚³ None
        """
        news_lower = news_text.lower()

        # æŒ‰æ˜ å°„è¡¨é€ä¸€æª¢æŸ¥
        for keyword, commodity_dir in self.COMMODITY_MAP.items():
            if keyword in news_lower:
                # æª¢æŸ¥è©²å•†å“ç›®éŒ„æ˜¯å¦å­˜åœ¨
                if commodity_dir in self.available_commodities:
                    logger.debug(f"åŒ¹é…å•†å“ï¼š{keyword} -> {commodity_dir}")
                    return commodity_dir
                else:
                    logger.debug(f"å•†å“ {commodity_dir} ç›®éŒ„ä¸å­˜åœ¨ï¼Œå¿½ç•¥")

        return None

    def is_valid_commodity(self, commodity_dir: str) -> bool:
        """
        æª¢æŸ¥å•†å“ç›®éŒ„æ˜¯å¦æœ‰æ•ˆ

        åƒæ•¸ï¼š
            commodity_dir: å•†å“ç›®éŒ„åç¨±

        å›å‚³ï¼š
            æ˜¯å¦æœ‰æ•ˆ
        """
        return commodity_dir in self.available_commodities
```

**æ¸¬è©¦æ–¹å¼**ï¼š

```python
# åœ¨ Python REPL ä¸­æ¸¬è©¦
from src.crawler.commodity_mapper import CommodityMapper

mapper = CommodityMapper('markets')

# æ¸¬è©¦æå–å•†å“
print(mapper.extract_commodity("Gold prices surge to new high"))  # æ‡‰è¼¸å‡º: Gold
print(mapper.extract_commodity("Bitcoin breaks $100,000"))  # æ‡‰è¼¸å‡º: Bitcoin
print(mapper.extract_commodity("Random news about stocks"))  # æ‡‰è¼¸å‡º: None
```

#### 3. æ–°èå„²å­˜ç®¡ç†æ¨¡çµ„

**æª”æ¡ˆ**ï¼š`src/crawler/news_storage.py`

**åŠŸèƒ½**ï¼š
- å°‡æ–°èä¿å­˜åˆ° `markets/<å•†å“>/yyyymmdd.txt`
- è‡ªå‹•ç®¡ç†éå¢ ID
- æª¢æŸ¥æ–°èé‡è¤‡
- è™•ç†æª”æ¡ˆé–ï¼ˆè·¨å¹³å°ç›¸å®¹ï¼‰

**å®Œæ•´ç¨‹å¼ç¢¼**ï¼š

```python
"""
æ–°èå„²å­˜æ¨¡çµ„

è² è²¬å°‡æ–°èä¿å­˜åˆ° markets/<å•†å“>/yyyymmdd.txtï¼Œä¸¦ç®¡ç† IDã€‚
"""

from typing import Optional, Tuple
from pathlib import Path
from datetime import datetime
from loguru import logger


class NewsStorage:
    """
    æ–°èå„²å­˜ç®¡ç†å™¨

    è² è²¬å°‡æ–°èä¿å­˜åˆ°å°æ‡‰å•†å“ç›®éŒ„ï¼Œä¸¦ç®¡ç†éå¢ IDã€‚
    """

    def __init__(self, markets_dir: str = 'markets'):
        """
        åˆå§‹åŒ–å„²å­˜ç®¡ç†å™¨

        åƒæ•¸ï¼š
            markets_dir: markets ç›®éŒ„è·¯å¾‘
        """
        self.markets_dir = Path(markets_dir)
        self.markets_dir.mkdir(parents=True, exist_ok=True)

    def save_news(
        self,
        commodity_dir: str,
        news_text: str,
        date: Optional[datetime] = None
    ) -> Tuple[bool, int]:
        """
        ä¿å­˜æ–°èåˆ°æŒ‡å®šå•†å“ç›®éŒ„

        åƒæ•¸ï¼š
            commodity_dir: å•†å“ç›®éŒ„åç¨±ï¼ˆå¦‚ 'Gold'ï¼‰
            news_text: æ–°èæ–‡æœ¬ï¼ˆè‹±æ–‡åŸæ–‡ï¼‰
            date: æ—¥æœŸï¼ˆé è¨­ç‚ºç•¶å¤©ï¼‰

        å›å‚³ï¼š
            (æ˜¯å¦æˆåŠŸ, æ–°è ID)
        """
        if date is None:
            date = datetime.now()

        # ç¢ºä¿å•†å“ç›®éŒ„å­˜åœ¨
        commodity_path = self.markets_dir / commodity_dir
        commodity_path.mkdir(parents=True, exist_ok=True)

        # æª”æ¡ˆè·¯å¾‘
        date_str = date.strftime('%Y%m%d')
        file_path = commodity_path / f"{date_str}.txt"

        # å–å¾—ä¸‹ä¸€å€‹ ID
        next_id = self._get_next_id(file_path)

        # å¯«å…¥æ–°èï¼ˆé™„åŠ æ¨¡å¼ï¼‰
        try:
            with open(file_path, 'a', encoding='utf-8') as f:
                # Windows ä¸æ”¯æ´ fcntlï¼Œä½¿ç”¨ try-except åŒ…è£
                try:
                    import fcntl
                    fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                except (ImportError, OSError):
                    # Windows æˆ–ä¸æ”¯æ´æª”æ¡ˆé–çš„ç³»çµ±ï¼Œç›´æ¥å¯«å…¥
                    pass

                # å¯«å…¥æ ¼å¼ï¼š[ID] æ–°èå…§å®¹
                f.write(f"[{next_id}] {news_text}\n")
                f.write("-" * 80 + "\n")

                # è§£é–ï¼ˆè‹¥æœ‰é–ï¼‰
                try:
                    import fcntl
                    fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                except (ImportError, OSError):
                    pass

            logger.info(f"æ–°èå·²ä¿å­˜ï¼š{file_path} (ID: {next_id})")
            return True, next_id

        except Exception as e:
            logger.error(f"ä¿å­˜æ–°èå¤±æ•—ï¼š{e}")
            return False, -1

    def _get_next_id(self, file_path: Path) -> int:
        """
        å–å¾—æª”æ¡ˆä¸­çš„ä¸‹ä¸€å€‹ ID

        åƒæ•¸ï¼š
            file_path: æª”æ¡ˆè·¯å¾‘

        å›å‚³ï¼š
            ä¸‹ä¸€å€‹ IDï¼ˆå¾ 1 é–‹å§‹ï¼‰
        """
        if not file_path.exists():
            return 1

        # è®€å–æª”æ¡ˆï¼Œè¨ˆç®—ç¾æœ‰ ID æ•¸é‡
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # çµ±è¨ˆ [ID] å‡ºç¾æ¬¡æ•¸
            id_count = sum(1 for line in lines if line.strip().startswith('['))
            return id_count + 1

        except Exception as e:
            logger.warning(f"è®€å–æª”æ¡ˆå¤±æ•—ï¼ŒID å¾ 1 é–‹å§‹ï¼š{e}")
            return 1

    def check_duplicate(
        self,
        commodity_dir: str,
        news_text: str,
        date: Optional[datetime] = None
    ) -> bool:
        """
        æª¢æŸ¥æ–°èæ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰

        åƒæ•¸ï¼š
            commodity_dir: å•†å“ç›®éŒ„åç¨±
            news_text: æ–°èæ–‡æœ¬
            date: æ—¥æœŸ

        å›å‚³ï¼š
            æ˜¯å¦é‡è¤‡
        """
        if date is None:
            date = datetime.now()

        date_str = date.strftime('%Y%m%d')
        file_path = self.markets_dir / commodity_dir / f"{date_str}.txt"

        if not file_path.exists():
            return False

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # ç°¡å–®çš„å­—ä¸²åŒ…å«æª¢æŸ¥
            # ç§»é™¤ news_text å‰å¾Œç©ºç™½ï¼Œä¸¦æª¢æŸ¥æ˜¯å¦å·²åœ¨æª”æ¡ˆä¸­
            return news_text.strip() in content

        except Exception as e:
            logger.warning(f"æª¢æŸ¥é‡è¤‡æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return False
```

**æ¸¬è©¦æ–¹å¼**ï¼š

```python
# åœ¨ Python REPL ä¸­æ¸¬è©¦
from src.crawler.news_storage import NewsStorage

storage = NewsStorage('markets')

# æ¸¬è©¦ä¿å­˜æ–°è
success, news_id = storage.save_news('Gold', 'Gold prices surge to new high')
print(f"ä¿å­˜æˆåŠŸ: {success}, ID: {news_id}")

# æ¸¬è©¦é‡è¤‡æª¢æŸ¥
is_dup = storage.check_duplicate('Gold', 'Gold prices surge to new high')
print(f"æ˜¯å¦é‡è¤‡: {is_dup}")
```

### æˆåŠŸæ¨™æº–

#### è‡ªå‹•åŒ–é©—è­‰
- [ ] `src/crawler/config.py` å­˜åœ¨ä¸”å¯æ­£å¸¸å°å…¥
- [ ] `src/crawler/commodity_mapper.py` å­˜åœ¨ä¸”å¯æ­£å¸¸å°å…¥
- [ ] `src/crawler/news_storage.py` å­˜åœ¨ä¸”å¯æ­£å¸¸å°å…¥
- [ ] åœ¨ Python REPL ä¸­å¯æˆåŠŸåŸ·è¡Œä¸Šè¿°æ¸¬è©¦ç¨‹å¼ç¢¼

#### æ‰‹å‹•é©—è­‰
- [ ] `CrawlerConfig.from_env()` èƒ½æ­£ç¢ºè¼‰å…¥ç’°å¢ƒè®Šæ•¸
- [ ] `CommodityMapper` èƒ½æ­£ç¢ºåŒ¹é…å•†å“ï¼ˆæ¸¬è©¦ Gold, Bitcoin, ç„¡åŒ¹é…æ¡ˆä¾‹ï¼‰
- [ ] `NewsStorage` èƒ½æ­£ç¢ºä¿å­˜æ–°èåˆ°æª”æ¡ˆï¼ŒID éå¢æ­£å¸¸
- [ ] é‡è¤‡æ–°èè¢«æ­£ç¢ºè­˜åˆ¥

**å¯¦ä½œæç¤º**ï¼š
- æ­¤éšæ®µè‘—é‡æ–¼åŸºç¤æ¨¡çµ„ï¼Œä¸æ¶‰åŠç¶²è·¯è«‹æ±‚
- å¯åœ¨æœ¬åœ°æ¸¬è©¦ï¼Œç„¡éœ€å•Ÿå‹• Bot
- é ä¼°æ™‚é–“ï¼š1-1.5 å°æ™‚

---

## éšæ®µä¸‰ï¼šçˆ¬èŸ²æ ¸å¿ƒæ¨¡çµ„

### æ¦‚è¿°

å¯¦ä½œæ–°èçˆ¬èŸ²çš„æ ¸å¿ƒåŠŸèƒ½ï¼šHTML æŠ“å–ã€è§£æã€å•†å“æå–ã€æ–°èå„²å­˜ã€‚é€™æ˜¯æ•´å€‹ç³»çµ±çš„å¿ƒè‡Ÿã€‚

### é—œéµå‰ç½®æ­¥é©Ÿï¼šHTML çµæ§‹åˆ†æ

**âš ï¸ é‡è¦**ï¼šåœ¨å¯¦ä½œä¹‹å‰ï¼Œå¿…é ˆæ‰‹å‹•åˆ†æ tradingeconomics.com çš„å¯¦éš› HTML çµæ§‹ã€‚

**åˆ†ææ­¥é©Ÿ**ï¼š

1. ä½¿ç”¨ç€è¦½å™¨è¨ªå• `https://tradingeconomics.com/stream?c=commodity`
2. æŒ‰ F12 æ‰“é–‹é–‹ç™¼è€…å·¥å…·
3. ä½¿ç”¨ã€Œå…ƒç´ é¸æ“‡å™¨ã€ï¼ˆæ¸¸æ¨™åœ–ç¤ºï¼‰é»é¸æ–°èé …ç›®
4. è¨˜éŒ„ä»¥ä¸‹è³‡è¨Šï¼š
   - æ–°èå®¹å™¨çš„ CSS é¡åˆ¥æˆ– ID
   - æ¨™é¡Œå…ƒç´ çš„é¸æ“‡å™¨
   - å…§å®¹å…ƒç´ çš„é¸æ“‡å™¨
   - æ™‚é–“å…ƒç´ çš„é¸æ“‡å™¨ï¼ˆå¦‚æœ‰ï¼‰

**ç¯„ä¾‹è¨˜éŒ„è¡¨**ï¼ˆéœ€æ ¹æ“šå¯¦éš›å¡«å¯«ï¼‰ï¼š

```
æ–°èå®¹å™¨: div.stream-item
æ¨™é¡Œå…ƒç´ : h3.stream-title
å…§å®¹å…ƒç´ : p.stream-content
æ™‚é–“å…ƒç´ : time (datetime å±¬æ€§)
```

### éœ€è¦å‰µå»ºçš„æª”æ¡ˆ

#### 1. æ–°èçˆ¬èŸ²æ ¸å¿ƒæ¨¡çµ„

**æª”æ¡ˆ**ï¼š`src/crawler/news_crawler.py`

**åŠŸèƒ½**ï¼š
- å¾ç›®æ¨™ç¶²ç«™æŠ“å– HTML
- è§£ææ–°èåˆ—è¡¨
- æå–å•†å“åç¨±ä¸¦å„²å­˜
- é˜²çˆ¬èŸ²ç­–ç•¥ï¼ˆå»¶é²ã€User-Agent è¼ªæ›ï¼‰

**å®Œæ•´ç¨‹å¼ç¢¼**ï¼š

```python
"""
æ–°èçˆ¬èŸ²æ ¸å¿ƒæ¨¡çµ„

è² è²¬å¾ç›®æ¨™ç¶²ç«™æŠ“å–å•†å“æ–°èã€‚
"""

from typing import List, Dict, Optional
import random
import asyncio
import httpx
from bs4 import BeautifulSoup
from loguru import logger
from datetime import datetime

from .config import CrawlerConfig
from .commodity_mapper import CommodityMapper
from .news_storage import NewsStorage


# User-Agent åˆ—è¡¨
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]


class NewsCrawler:
    """
    å•†å“æ–°èçˆ¬èŸ²

    è² è²¬å¾ tradingeconomics.com æŠ“å–å•†å“æ–°èã€‚
    """

    def __init__(self, config: CrawlerConfig):
        """
        åˆå§‹åŒ–çˆ¬èŸ²

        åƒæ•¸ï¼š
            config: çˆ¬èŸ²é…ç½®
        """
        self.config = config
        self.mapper = CommodityMapper(config.markets_dir)
        self.storage = NewsStorage(config.markets_dir)

        logger.info("æ–°èçˆ¬èŸ²åˆå§‹åŒ–å®Œæˆ")

    async def fetch_page(self) -> Optional[str]:
        """
        æŠ“å–ç›®æ¨™ç¶²é  HTML

        å›å‚³ï¼š
            HTML å…§å®¹ï¼Œå¤±æ•—æ™‚å›å‚³ None
        """
        # éš¨æ©Ÿå»¶é²ï¼ˆ0.5-2 ç§’ï¼‰
        delay = random.uniform(0.5, 2.0)
        logger.debug(f"è«‹æ±‚å‰å»¶é² {delay:.2f} ç§’")
        await asyncio.sleep(delay)

        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://tradingeconomics.com/',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

        try:
            async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                logger.info(f"æ­£åœ¨æŠ“å–ï¼š{self.config.target_url}")
                response = await client.get(self.config.target_url, headers=headers)

                if response.status_code == 200:
                    logger.info("ç¶²é æŠ“å–æˆåŠŸ")
                    return response.text
                else:
                    logger.error(f"ç¶²é æŠ“å–å¤±æ•—ï¼šHTTP {response.status_code}")
                    return None

        except Exception as e:
            logger.error(f"ç¶²é æŠ“å–ç•°å¸¸ï¼š{e}")
            return None

    def parse_news(self, html: str) -> List[Dict[str, str]]:
        """
        è§£æ HTMLï¼Œæå–æ–°èåˆ—è¡¨

        âš ï¸ é‡è¦ï¼šæ­¤å‡½å¼çš„ CSS é¸æ“‡å™¨éœ€è¦æ ¹æ“šå¯¦éš›ç¶²ç«™çµæ§‹èª¿æ•´ï¼

        åƒæ•¸ï¼š
            html: ç¶²é  HTML å…§å®¹

        å›å‚³ï¼š
            æ–°èåˆ—è¡¨ [{'title': ..., 'content': ..., 'full_text': ..., 'time': ...}, ...]
        """
        soup = BeautifulSoup(html, 'lxml')
        news_list = []

        # TODO: æ ¹æ“šå¯¦éš›ç¶²ç«™çµæ§‹èª¿æ•´é¸æ“‡å™¨
        # ä»¥ä¸‹ç‚ºç¤ºæ„ç¨‹å¼ç¢¼ï¼Œéœ€æ ¹æ“š tradingeconomics.com çš„å¯¦éš› HTML çµæ§‹ä¿®æ”¹
        try:
            # ç¯„ä¾‹ï¼šå‡è¨­æ–°èåœ¨ <div class="stream-item"> ä¸­
            # âš ï¸ è«‹æ ¹æ“šå¯¦éš› HTML çµæ§‹ä¿®æ”¹æ­¤é¸æ“‡å™¨
            items = soup.select('div.stream-item')

            if not items:
                # å˜—è©¦å‚™ç”¨é¸æ“‡å™¨
                logger.warning("ä¸»è¦é¸æ“‡å™¨ 'div.stream-item' ç„¡åŒ¹é…ï¼Œå˜—è©¦å‚™ç”¨é¸æ“‡å™¨")
                items = soup.select('article.news-item')

            if not items:
                logger.warning("æ‰€æœ‰é¸æ“‡å™¨éƒ½ç„¡æ³•åŒ¹é…ï¼Œè«‹æª¢æŸ¥ç¶²ç«™çµæ§‹")

            for item in items:
                try:
                    # æå–æ¨™é¡Œ
                    # âš ï¸ è«‹æ ¹æ“šå¯¦éš› HTML çµæ§‹ä¿®æ”¹æ­¤é¸æ“‡å™¨
                    title_elem = item.select_one('h3, h2, .title')
                    title = title_elem.get_text(strip=True) if title_elem else ''

                    # æå–å…§å®¹
                    # âš ï¸ è«‹æ ¹æ“šå¯¦éš› HTML çµæ§‹ä¿®æ”¹æ­¤é¸æ“‡å™¨
                    content_elem = item.select_one('p, .content, .description')
                    content = content_elem.get_text(strip=True) if content_elem else ''

                    # æå–æ™‚é–“
                    # âš ï¸ è«‹æ ¹æ“šå¯¦éš› HTML çµæ§‹ä¿®æ”¹æ­¤é¸æ“‡å™¨
                    time_elem = item.select_one('time, .timestamp, .date')
                    time_str = ''
                    if time_elem:
                        time_str = time_elem.get('datetime', '') or time_elem.get_text(strip=True)

                    # çµ„åˆå®Œæ•´æ–‡æœ¬
                    full_text = f"{title}\n{content}" if content else title

                    if full_text:
                        news_list.append({
                            'title': title,
                            'content': content,
                            'full_text': full_text,
                            'time': time_str
                        })

                except Exception as e:
                    logger.warning(f"è§£æå–®å‰‡æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue

            logger.info(f"æˆåŠŸè§£æ {len(news_list)} å‰‡æ–°è")

        except Exception as e:
            logger.error(f"è§£æ HTML æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

        return news_list

    async def process_and_save(
        self,
        news_list: List[Dict[str, str]]
    ) -> List[Dict[str, any]]:
        """
        è™•ç†æ–°èåˆ—è¡¨ä¸¦ä¿å­˜

        åƒæ•¸ï¼š
            news_list: è§£æå¾Œçš„æ–°èåˆ—è¡¨

        å›å‚³ï¼š
            å·²ä¿å­˜çš„æ–°èåˆ—è¡¨ï¼ˆåŒ…å«å•†å“å’Œ ID è³‡è¨Šï¼‰
        """
        saved_news = []

        for news in news_list:
            full_text = news['full_text']

            # æå–å•†å“
            commodity = self.mapper.extract_commodity(full_text)
            if not commodity:
                logger.debug(f"æ–°èæœªåŒ¹é…ä»»ä½•å•†å“ï¼Œå¿½ç•¥ï¼š{full_text[:50]}...")
                continue

            # æª¢æŸ¥é‡è¤‡
            if self.storage.check_duplicate(commodity, full_text):
                logger.debug(f"æ–°èé‡è¤‡ï¼Œå¿½ç•¥ï¼š{full_text[:50]}...")
                continue

            # ä¿å­˜æ–°è
            success, news_id = self.storage.save_news(commodity, full_text)

            if success:
                saved_news.append({
                    'commodity': commodity,
                    'news_id': news_id,
                    'text': full_text,
                    'time': news.get('time', '')
                })
                logger.info(f"æ–°èå·²ä¿å­˜ï¼š{commodity} ID={news_id}")

        return saved_news

    async def crawl(self) -> List[Dict[str, any]]:
        """
        åŸ·è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹

        å›å‚³ï¼š
            å·²ä¿å­˜çš„æ–°èåˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("é–‹å§‹çˆ¬å–å•†å“æ–°è")
        logger.info("=" * 60)

        # 1. æŠ“å–ç¶²é 
        html = await self.fetch_page()
        if not html:
            logger.error("ç¶²é æŠ“å–å¤±æ•—ï¼Œæœ¬æ¬¡çˆ¬å–çµæŸ")
            return []

        # 2. è§£ææ–°è
        news_list = self.parse_news(html)
        if not news_list:
            logger.warning("æœªè§£æåˆ°ä»»ä½•æ–°è")
            return []

        # 3. è™•ç†ä¸¦ä¿å­˜
        saved_news = await self.process_and_save(news_list)

        logger.info("=" * 60)
        logger.info(f"çˆ¬å–å®Œæˆï¼šå…±ä¿å­˜ {len(saved_news)} å‰‡æ–°è")
        logger.info("=" * 60)

        return saved_news
```

### HTML çµæ§‹èª¿æ•´æŒ‡å¼•

**åœ¨ `parse_news()` å‡½å¼ä¸­éœ€è¦èª¿æ•´çš„åœ°æ–¹**ï¼ˆæ¨™è¨˜ç‚º `âš ï¸`ï¼‰ï¼š

1. **æ–°èå®¹å™¨é¸æ“‡å™¨**ï¼ˆç¬¬ 158 è¡Œï¼‰
   ```python
   items = soup.select('div.stream-item')  # â† æ ¹æ“šå¯¦éš›ä¿®æ”¹
   ```

2. **æ¨™é¡Œé¸æ“‡å™¨**ï¼ˆç¬¬ 168 è¡Œï¼‰
   ```python
   title_elem = item.select_one('h3, h2, .title')  # â† æ ¹æ“šå¯¦éš›ä¿®æ”¹
   ```

3. **å…§å®¹é¸æ“‡å™¨**ï¼ˆç¬¬ 173 è¡Œï¼‰
   ```python
   content_elem = item.select_one('p, .content, .description')  # â† æ ¹æ“šå¯¦éš›ä¿®æ”¹
   ```

4. **æ™‚é–“é¸æ“‡å™¨**ï¼ˆç¬¬ 178 è¡Œï¼‰
   ```python
   time_elem = item.select_one('time, .timestamp, .date')  # â† æ ¹æ“šå¯¦éš›ä¿®æ”¹
   ```

### æ¸¬è©¦æ–¹å¼

#### ç¨ç«‹æ¸¬è©¦çˆ¬èŸ²

å»ºç«‹æ¸¬è©¦è…³æœ¬ `test_crawler_standalone.py`ï¼ˆå°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰ï¼š

```python
"""
ç¨ç«‹æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½
"""
import asyncio
from src.crawler.config import CrawlerConfig
from src.crawler.news_crawler import NewsCrawler
from loguru import logger

async def test_crawl():
    """æ¸¬è©¦çˆ¬å–åŠŸèƒ½"""
    # è¼‰å…¥é…ç½®
    config = CrawlerConfig.from_env()

    # å‰µå»ºçˆ¬èŸ²
    crawler = NewsCrawler(config)

    # åŸ·è¡Œçˆ¬å–
    saved_news = await crawler.crawl()

    # é¡¯ç¤ºçµæœ
    logger.info(f"æ¸¬è©¦å®Œæˆï¼Œå…±ä¿å­˜ {len(saved_news)} å‰‡æ–°è")
    for news in saved_news:
        logger.info(f"  - {news['commodity']} (ID: {news['news_id']}): {news['text'][:50]}...")

if __name__ == '__main__':
    asyncio.run(test_crawl())
```

**åŸ·è¡Œæ¸¬è©¦**ï¼š
```bash
python test_crawler_standalone.py
```

### æˆåŠŸæ¨™æº–

#### è‡ªå‹•åŒ–é©—è­‰
- [ ] `src/crawler/news_crawler.py` å­˜åœ¨ä¸”å¯æ­£å¸¸å°å…¥
- [ ] åŸ·è¡Œ `test_crawler_standalone.py` ä¸å ±éŒ¯
- [ ] ç¶²é æŠ“å–æˆåŠŸï¼ˆHTTP 200ï¼‰

#### æ‰‹å‹•é©—è­‰
- [ ] `parse_news()` èƒ½æ­£ç¢ºè§£æå‡ºæ–°èåˆ—è¡¨ï¼ˆè‡³å°‘ 1 å‰‡ï¼‰
- [ ] æ–°èè¢«æ­£ç¢ºä¿å­˜åˆ° `markets/<å•†å“>/yyyymmdd.txt`
- [ ] æª”æ¡ˆä¸­çš„ ID æ­£ç¢ºéå¢ï¼ˆ[1], [2], [3]...ï¼‰
- [ ] é‡è¤‡æ–°èä¸æœƒè¢«é‡è¤‡ä¿å­˜
- [ ] æ—¥èªŒé¡¯ç¤ºå•†å“åŒ¹é…è³‡è¨Šï¼ˆå¦‚ "åŒ¹é…å•†å“ï¼šgold -> Gold"ï¼‰

**å¯¦ä½œæç¤º**ï¼š
- æ­¤éšæ®µæ˜¯æ ¸å¿ƒåŠŸèƒ½ï¼Œéœ€è¦è€å¿ƒèª¿æ•´ CSS é¸æ“‡å™¨
- å»ºè­°å…ˆåœ¨ç€è¦½å™¨ä¸­é©—è­‰é¸æ“‡å™¨çš„æ­£ç¢ºæ€§
- è‹¥è§£æå¤±æ•—ï¼Œæª¢æŸ¥æ—¥èªŒä¸­çš„éŒ¯èª¤è¨Šæ¯
- é ä¼°æ™‚é–“ï¼š1.5-2 å°æ™‚ï¼ˆåŒ…å« HTML çµæ§‹åˆ†æï¼‰

**âš ï¸ éšæ®µä¸‰å®Œæˆæª¢æŸ¥é»**ï¼šåœ¨é€²å…¥éšæ®µå››ä¹‹å‰ï¼Œè«‹ç¢ºä¿çˆ¬èŸ²èƒ½ç¨ç«‹é‹è¡Œä¸¦æ­£ç¢ºå„²å­˜æ–°èåˆ°æª”æ¡ˆã€‚

---

## éšæ®µå››ï¼šå®šæ™‚ä»»å‹™æ•´åˆ

### æ¦‚è¿°

å¯¦ä½œå®šæ™‚ä»»å‹™èª¿åº¦å™¨ï¼Œå°‡çˆ¬èŸ²æ•´åˆåˆ° Telegram Bot çš„ç”Ÿå‘½é€±æœŸä¸­ï¼Œå¯¦ç¾è‡ªå‹•åŒ–å®šæ™‚çˆ¬å–ã€‚

### éœ€è¦å‰µå»º/ä¿®æ”¹çš„æª”æ¡ˆ

#### 1. çˆ¬èŸ²èª¿åº¦å™¨æ¨¡çµ„

**æª”æ¡ˆ**ï¼š`src/crawler/scheduler.py`

**åŠŸèƒ½**ï¼š
- ä½¿ç”¨ APScheduler ç®¡ç†å®šæ™‚ä»»å‹™
- æ•´åˆ Telegram é€šçŸ¥åŠŸèƒ½
- èˆ‡ Bot ç”Ÿå‘½é€±æœŸåŒæ­¥

**å®Œæ•´ç¨‹å¼ç¢¼**ï¼š

```python
"""
çˆ¬èŸ²å®šæ™‚ä»»å‹™ç®¡ç†æ¨¡çµ„

è² è²¬å•Ÿå‹•å’Œç®¡ç†æ–°èçˆ¬èŸ²çš„å®šæ™‚ä»»å‹™ã€‚
"""

from typing import Optional
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from loguru import logger
from telegram.ext import Application

from .config import CrawlerConfig
from .news_crawler import NewsCrawler


class CrawlerScheduler:
    """
    çˆ¬èŸ²å®šæ™‚ä»»å‹™ç®¡ç†å™¨

    ä½¿ç”¨ APScheduler ç®¡ç†çˆ¬èŸ²çš„å®šæ™‚åŸ·è¡Œã€‚
    """

    def __init__(
        self,
        config: CrawlerConfig,
        telegram_app: Optional[Application] = None
    ):
        """
        åˆå§‹åŒ–èª¿åº¦å™¨

        åƒæ•¸ï¼š
            config: çˆ¬èŸ²é…ç½®
            telegram_app: Telegram Application å¯¦ä¾‹ï¼ˆç”¨æ–¼ç™¼é€é€šçŸ¥ï¼‰
        """
        self.config = config
        self.telegram_app = telegram_app
        self.crawler = NewsCrawler(config)
        self.scheduler = AsyncIOScheduler()

        logger.info("çˆ¬èŸ²èª¿åº¦å™¨åˆå§‹åŒ–å®Œæˆ")

    async def _crawl_and_notify(self):
        """
        çˆ¬å–æ–°èä¸¦ç™¼é€ Telegram é€šçŸ¥
        """
        try:
            # åŸ·è¡Œçˆ¬å–
            saved_news = await self.crawler.crawl()

            # ç™¼é€ Telegram é€šçŸ¥
            if saved_news and self.telegram_app and self.config.telegram_notify_groups:
                await self._send_telegram_notifications(saved_news)

        except Exception as e:
            logger.exception(f"çˆ¬èŸ²åŸ·è¡Œå¤±æ•—ï¼š{e}")

    async def _send_telegram_notifications(self, saved_news: list):
        """
        ç™¼é€ Telegram é€šçŸ¥

        åƒæ•¸ï¼š
            saved_news: å·²ä¿å­˜çš„æ–°èåˆ—è¡¨
        """
        for news in saved_news:
            # æ ¼å¼åŒ–è¨Šæ¯
            message = self._format_news_message(news)

            # ç™¼é€åˆ°æ‰€æœ‰é…ç½®çš„ç¾¤çµ„
            for group_id in self.config.telegram_notify_groups:
                try:
                    await self.telegram_app.bot.send_message(
                        chat_id=group_id,
                        text=message,
                        parse_mode='Markdown'
                    )
                    logger.info(f"å·²ç™¼é€é€šçŸ¥åˆ°ç¾¤çµ„ {group_id}")

                except Exception as e:
                    logger.error(f"ç™¼é€é€šçŸ¥åˆ°ç¾¤çµ„ {group_id} å¤±æ•—ï¼š{e}")

    def _format_news_message(self, news: dict) -> str:
        """
        æ ¼å¼åŒ–æ–°èè¨Šæ¯

        åƒæ•¸ï¼š
            news: æ–°èè³‡æ–™

        å›å‚³ï¼š
            æ ¼å¼åŒ–å¾Œçš„ Markdown è¨Šæ¯
        """
        commodity = news['commodity']
        news_id = news['news_id']
        text = news['text']
        time = news.get('time', 'N/A')

        # é™åˆ¶æ–‡æœ¬é•·åº¦ï¼ˆTelegram å–®å‰‡è¨Šæ¯æœ€å¤š 4096 å­—å…ƒï¼‰
        max_length = 3000
        if len(text) > max_length:
            text = text[:max_length] + "..."

        message = (
            f"ğŸ“° **{commodity} å•†å“æ–°è** (ID: {news_id})\n\n"
            f"{text}\n\n"
            f"â° {time}"
        )

        return message

    def start(self):
        """
        å•Ÿå‹•å®šæ™‚ä»»å‹™
        """
        if not self.config.enabled:
            logger.info("çˆ¬èŸ²å·²åœç”¨ï¼ˆCRAWLER_ENABLED=falseï¼‰ï¼Œä¸å•Ÿå‹•å®šæ™‚ä»»å‹™")
            return

        # è¨ˆç®— jitterï¼ˆéš¨æ©ŸåŒ–ç¯„åœï¼‰
        jitter = self.config.interval_jitter_seconds

        # æ–°å¢ä»»å‹™
        self.scheduler.add_job(
            self._crawl_and_notify,
            trigger=IntervalTrigger(
                minutes=self.config.crawl_interval_minutes,
                jitter=jitter
            ),
            id='news_crawler',
            name='å•†å“æ–°èçˆ¬èŸ²',
            replace_existing=True
        )

        # å•Ÿå‹•èª¿åº¦å™¨
        self.scheduler.start()

        logger.info(
            f"çˆ¬èŸ²å®šæ™‚ä»»å‹™å·²å•Ÿå‹•ï¼šæ¯ {self.config.crawl_interval_minutes} åˆ†é˜ "
            f"(Â±{jitter} ç§’) åŸ·è¡Œä¸€æ¬¡"
        )

    def stop(self):
        """
        åœæ­¢å®šæ™‚ä»»å‹™
        """
        if self.scheduler.running:
            self.scheduler.shutdown(wait=False)
            logger.info("çˆ¬èŸ²å®šæ™‚ä»»å‹™å·²åœæ­¢")
```

#### 2. æ•´åˆåˆ° Telegram Bot

**æª”æ¡ˆ**ï¼š`src/bot/telegram_bot.py`

**ä¿®æ”¹å…§å®¹**ï¼šåœ¨æª”æ¡ˆé–‹é ­å°å…¥çˆ¬èŸ²æ¨¡çµ„ï¼Œä¸¦åœ¨ `__init__()` å’Œç”Ÿå‘½é€±æœŸé‰¤å­ä¸­æ•´åˆçˆ¬èŸ²ã€‚

**ä¿®æ”¹æ­¥é©Ÿ**ï¼š

**æ­¥é©Ÿ 1**ï¼šåœ¨æª”æ¡ˆé–‹é ­æ–°å¢å°å…¥ï¼ˆç´„ç¬¬ 19 è¡Œä¹‹å¾Œï¼‰

```python
from .config import BotConfig
from .handlers import (
    start_command,
    help_command,
    status_command,
    handle_message,
    handle_error
)

# æ–°å¢ï¼šå°å…¥çˆ¬èŸ²æ¨¡çµ„
from src.crawler.config import CrawlerConfig
from src.crawler.scheduler import CrawlerScheduler
```

**æ­¥é©Ÿ 2**ï¼šåœ¨ `__init__()` æ–¹æ³•ä¸­åˆå§‹åŒ–çˆ¬èŸ²èª¿åº¦å™¨ï¼ˆç´„ç¬¬ 58 è¡Œä¹‹å¾Œï¼‰

```python
    def __init__(self, config: BotConfig):
        """
        åˆå§‹åŒ– Bot

        åƒæ•¸ï¼š
            config: Bot è¨­å®š
        """
        self.config = config

        # å»ºç«‹ Application
        self.application = (
            Application.builder()
            .token(config.telegram_bot_token)
            .build()
        )

        # å„²å­˜è¨­å®šåˆ° bot_data
        self.application.bot_data['config'] = config

        # è¨»å†Šè™•ç†å™¨
        self._register_handlers()

        # æ–°å¢ï¼šåˆå§‹åŒ–çˆ¬èŸ²èª¿åº¦å™¨
        crawler_config = CrawlerConfig.from_env()
        self.crawler_scheduler = CrawlerScheduler(
            config=crawler_config,
            telegram_app=self.application
        )

        logger.info("Telegram Bot åˆå§‹åŒ–å®Œæˆ")
```

**æ­¥é©Ÿ 3**ï¼šåœ¨ `_post_init()` æ–¹æ³•ä¸­å•Ÿå‹•çˆ¬èŸ²ï¼ˆç´„ç¬¬ 95 è¡Œä¹‹å¾Œï¼‰

```python
    async def _post_init(self, application: Application):
        """
        åˆå§‹åŒ–å¾Œå›èª¿

        åœ¨ Bot å•Ÿå‹•å¾ŒåŸ·è¡Œçš„åˆå§‹åŒ–ä»»å‹™ã€‚
        """
        logger.info("Bot å•Ÿå‹•å¾Œåˆå§‹åŒ–...")

        # å–å¾— Bot è³‡è¨Š
        bot = await application.bot.get_me()
        logger.info(f"Bot ç”¨æˆ¶åï¼š@{bot.username}")
        logger.info(f"Bot IDï¼š{bot.id}")

        # ç™¼é€é–‹å¼µè¨Šæ¯åˆ°æ‰€æœ‰é…ç½®çš„ç¾¤çµ„
        await self._send_startup_message(application)

        # æ–°å¢ï¼šå•Ÿå‹•çˆ¬èŸ²å®šæ™‚ä»»å‹™
        self.crawler_scheduler.start()
        logger.info("çˆ¬èŸ²å®šæ™‚ä»»å‹™å·²æ•´åˆåˆ° Bot ç”Ÿå‘½é€±æœŸ")
```

**æ­¥é©Ÿ 4**ï¼šåœ¨ `_post_shutdown()` æ–¹æ³•ä¸­åœæ­¢çˆ¬èŸ²ï¼ˆç´„ç¬¬ 128 è¡Œä¹‹å¾Œï¼‰

```python
    async def _post_shutdown(self, application: Application):
        """
        é—œé–‰å¾Œå›èª¿

        åœ¨ Bot é—œé–‰å‰åŸ·è¡Œçš„æ¸…ç†ä»»å‹™ã€‚
        """
        logger.info("Bot æ­£åœ¨é—œé–‰...")

        # æ–°å¢ï¼šåœæ­¢çˆ¬èŸ²å®šæ™‚ä»»å‹™
        self.crawler_scheduler.stop()
        logger.info("çˆ¬èŸ²å®šæ™‚ä»»å‹™å·²åœæ­¢")
```

### æ¸¬è©¦æ–¹å¼

#### å•Ÿå‹• Bot ä¸¦é©—è­‰çˆ¬èŸ²æ•´åˆ

```bash
python scripts/run_bot.py
```

**é æœŸæ—¥èªŒè¼¸å‡º**ï¼š

```
2026-01-02 14:30:00 | INFO | çˆ¬èŸ²èª¿åº¦å™¨åˆå§‹åŒ–å®Œæˆ
2026-01-02 14:30:00 | INFO | Telegram Bot åˆå§‹åŒ–å®Œæˆ
2026-01-02 14:30:01 | INFO | Bot ç”¨æˆ¶åï¼š@YourBotName
2026-01-02 14:30:01 | INFO | Bot IDï¼š1234567890
2026-01-02 14:30:01 | INFO | å·²ç™¼é€é–‹å¼µè¨Šæ¯åˆ°ç¾¤çµ„ -1001234567890
2026-01-02 14:30:01 | INFO | çˆ¬èŸ²å®šæ™‚ä»»å‹™å·²å•Ÿå‹•ï¼šæ¯ 5 åˆ†é˜ (Â±15 ç§’) åŸ·è¡Œä¸€æ¬¡
2026-01-02 14:30:01 | INFO | çˆ¬èŸ²å®šæ™‚ä»»å‹™å·²æ•´åˆåˆ° Bot ç”Ÿå‘½é€±æœŸ
```

#### é©—è­‰å®šæ™‚ä»»å‹™è§¸ç™¼

ç­‰å¾…ç´„ 5 åˆ†é˜ï¼Œæ‡‰çœ‹åˆ°çˆ¬èŸ²è‡ªå‹•åŸ·è¡Œï¼š

```
2026-01-02 14:35:12 | INFO | =============================================================
2026-01-02 14:35:12 | INFO | é–‹å§‹çˆ¬å–å•†å“æ–°è
2026-01-02 14:35:12 | INFO | =============================================================
2026-01-02 14:35:13 | INFO | æ­£åœ¨æŠ“å–ï¼šhttps://tradingeconomics.com/stream?c=commodity
2026-01-02 14:35:14 | INFO | ç¶²é æŠ“å–æˆåŠŸ
2026-01-02 14:35:14 | INFO | æˆåŠŸè§£æ 10 å‰‡æ–°è
2026-01-02 14:35:14 | INFO | æ–°èå·²ä¿å­˜ï¼šGold ID=1
2026-01-02 14:35:14 | INFO | å·²ç™¼é€é€šçŸ¥åˆ°ç¾¤çµ„ -1001234567890
...
```

### æˆåŠŸæ¨™æº–

#### è‡ªå‹•åŒ–é©—è­‰
- [ ] `src/crawler/scheduler.py` å­˜åœ¨ä¸”å¯æ­£å¸¸å°å…¥
- [ ] `src/bot/telegram_bot.py` ä¿®æ”¹å¾Œç„¡èªæ³•éŒ¯èª¤
- [ ] åŸ·è¡Œ `python scripts/run_bot.py` å¯æ­£å¸¸å•Ÿå‹•
- [ ] æ—¥èªŒé¡¯ç¤ºã€Œçˆ¬èŸ²å®šæ™‚ä»»å‹™å·²å•Ÿå‹•ã€

#### æ‰‹å‹•é©—è­‰
- [ ] Bot å•Ÿå‹•å¾Œè‡ªå‹•å•Ÿå‹•çˆ¬èŸ²å®šæ™‚ä»»å‹™
- [ ] ç­‰å¾… 5 åˆ†é˜å¾Œï¼Œçˆ¬èŸ²è‡ªå‹•åŸ·è¡Œä¸€æ¬¡
- [ ] æ—¥èªŒé¡¯ç¤ºçˆ¬å–éç¨‹å’Œçµæœ
- [ ] è‹¥æœ‰æ–°æ–°èï¼ŒTelegram ç¾¤çµ„æ”¶åˆ°é€šçŸ¥
- [ ] ä½¿ç”¨ Ctrl+C åœæ­¢ Bot æ™‚ï¼Œæ—¥èªŒé¡¯ç¤ºã€Œçˆ¬èŸ²å®šæ™‚ä»»å‹™å·²åœæ­¢ã€

**å¯¦ä½œæç¤º**ï¼š
- æ­¤éšæ®µæ•´åˆäº† Bot å’Œçˆ¬èŸ²ï¼Œéœ€è¦ç¢ºä¿ä¸å½±éŸ¿ Bot åŸæœ‰åŠŸèƒ½
- å¯å…ˆè¨­å®š `CRAWLER_ENABLED=false` æ¸¬è©¦ Bot å•Ÿå‹•ï¼Œç¢ºèªç„¡å½±éŸ¿
- é ä¼°æ™‚é–“ï¼š30-45 åˆ†é˜

---

## éšæ®µäº”ï¼šTelegram é€šçŸ¥å„ªåŒ–èˆ‡æ¸¬è©¦

### æ¦‚è¿°

å„ªåŒ– Telegram é€šçŸ¥è¨Šæ¯æ ¼å¼ï¼Œæ–°å¢æ‰‹å‹•æ¸¬è©¦æŒ‡ä»¤ï¼Œé€²è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æ¸¬è©¦ã€‚

### éœ€è¦å‰µå»º/ä¿®æ”¹çš„æª”æ¡ˆ

#### 1. æ–°å¢æ‰‹å‹•æ¸¬è©¦æŒ‡ä»¤ï¼ˆé¸ç”¨ï¼‰

**æª”æ¡ˆ**ï¼š`src/bot/handlers.py`

**åŠŸèƒ½**ï¼šæ–°å¢ `/crawl_now` æŒ‡ä»¤ï¼Œè®“ç®¡ç†å“¡å¯ä»¥æ‰‹å‹•è§¸ç™¼ä¸€æ¬¡çˆ¬å–

**ä¿®æ”¹å…§å®¹**ï¼šåœ¨æª”æ¡ˆæœ«å°¾æ–°å¢

```python
async def crawl_now_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    /crawl_now æŒ‡ä»¤è™•ç†å™¨

    æ‰‹å‹•è§¸ç™¼ä¸€æ¬¡æ–°èçˆ¬å–ï¼ˆåƒ…é™ç®¡ç†å“¡ï¼‰
    """
    config = context.application.bot_data.get('config')

    # æª¢æŸ¥æ˜¯å¦åœ¨å…è¨±çš„ç¾¤çµ„ä¸­
    if not config or not config.is_allowed_group(update.effective_chat.id):
        return

    # æª¢æŸ¥æ˜¯å¦ç‚ºç®¡ç†å“¡
    chat_member = await update.effective_chat.get_member(update.effective_user.id)
    if chat_member.status not in ['creator', 'administrator']:
        await update.message.reply_text("âš ï¸ æ­¤æŒ‡ä»¤åƒ…é™ç¾¤çµ„ç®¡ç†å“¡ä½¿ç”¨")
        return

    await update.message.reply_text("ğŸ”„ æ­£åœ¨æ‰‹å‹•è§¸ç™¼æ–°èçˆ¬å–...")

    try:
        # å–å¾—çˆ¬èŸ²èª¿åº¦å™¨ï¼ˆå¾ Bot å¯¦ä¾‹ï¼‰
        # æ³¨æ„ï¼šéœ€è¦åœ¨ telegram_bot.py ä¸­å°‡ crawler_scheduler å„²å­˜åˆ° bot_data
        crawler_scheduler = context.application.bot_data.get('crawler_scheduler')

        if crawler_scheduler:
            await crawler_scheduler._crawl_and_notify()
            await update.message.reply_text("âœ… çˆ¬å–å®Œæˆï¼Œè«‹æŸ¥çœ‹ä¸Šæ–¹é€šçŸ¥")
        else:
            await update.message.reply_text("âŒ çˆ¬èŸ²æœªå•Ÿå‹•")

    except Exception as e:
        logger.error(f"æ‰‹å‹•çˆ¬å–å¤±æ•—ï¼š{e}")
        await update.message.reply_text(f"âŒ çˆ¬å–å¤±æ•—ï¼š{e}")
```

**æ•´åˆåˆ° Bot**ï¼š

**æª”æ¡ˆ**ï¼š`src/bot/telegram_bot.py`

**ä¿®æ”¹ `_register_handlers()` æ–¹æ³•**ï¼ˆç´„ç¬¬ 66 è¡Œï¼‰ï¼š

```python
    def _register_handlers(self):
        """è¨»å†Šæ‰€æœ‰è¨Šæ¯è™•ç†å™¨"""

        # æŒ‡ä»¤è™•ç†å™¨ï¼ˆç¾¤çµ„ä¸­çš„æŒ‡ä»¤ï¼‰
        self.application.add_handler(CommandHandler("start", start_command))
        self.application.add_handler(CommandHandler("help", help_command))
        self.application.add_handler(CommandHandler("status", status_command))

        # æ–°å¢ï¼šæ‰‹å‹•çˆ¬å–æŒ‡ä»¤ï¼ˆé¸ç”¨ï¼‰
        from .handlers import crawl_now_command
        self.application.add_handler(CommandHandler("crawl_now", crawl_now_command))

        # è¨Šæ¯è™•ç†å™¨ï¼šåªæ¥æ”¶ç¾¤çµ„ä¸­çš„éæŒ‡ä»¤æ–‡å­—è¨Šæ¯
        self.application.add_handler(
            MessageHandler(
                filters.TEXT & ~filters.COMMAND & filters.ChatType.GROUPS,
                handle_message
            )
        )

        # éŒ¯èª¤è™•ç†å™¨
        self.application.add_error_handler(handle_error)

        logger.info("æ‰€æœ‰è™•ç†å™¨è¨»å†Šå®Œæˆï¼ˆç´”ç¾¤çµ„æ¨¡å¼ï¼‰")
```

**ä¿®æ”¹ `__init__()` æ–¹æ³•**ï¼Œå°‡ `crawler_scheduler` å„²å­˜åˆ° `bot_data`ï¼ˆç´„ç¬¬ 76 è¡Œï¼‰ï¼š

```python
        # æ–°å¢ï¼šåˆå§‹åŒ–çˆ¬èŸ²èª¿åº¦å™¨
        crawler_config = CrawlerConfig.from_env()
        self.crawler_scheduler = CrawlerScheduler(
            config=crawler_config,
            telegram_app=self.application
        )

        # æ–°å¢ï¼šå„²å­˜åˆ° bot_dataï¼Œä¾›æŒ‡ä»¤è™•ç†å™¨ä½¿ç”¨
        self.application.bot_data['crawler_scheduler'] = self.crawler_scheduler
```

#### 2. å„ªåŒ–é€šçŸ¥è¨Šæ¯æ ¼å¼

**æª”æ¡ˆ**ï¼š`src/crawler/scheduler.py`

**ä¿®æ”¹ `_format_news_message()` æ–¹æ³•**ï¼ˆç´„ç¬¬ 76 è¡Œï¼‰ï¼š

```python
    def _format_news_message(self, news: dict) -> str:
        """
        æ ¼å¼åŒ–æ–°èè¨Šæ¯

        åƒæ•¸ï¼š
            news: æ–°èè³‡æ–™

        å›å‚³ï¼š
            æ ¼å¼åŒ–å¾Œçš„ Markdown è¨Šæ¯
        """
        commodity = news['commodity']
        news_id = news['news_id']
        text = news['text']
        time = news.get('time', 'N/A')

        # é™åˆ¶æ–‡æœ¬é•·åº¦ï¼ˆTelegram å–®å‰‡è¨Šæ¯æœ€å¤š 4096 å­—å…ƒï¼‰
        max_length = 3000
        if len(text) > max_length:
            text = text[:max_length] + "..."

        # æ ¹æ“šå•†å“é¡å‹é¸æ“‡è¡¨æƒ…ç¬¦è™Ÿ
        emoji_map = {
            'Gold': 'ğŸ¥‡',
            'Silver': 'ğŸ¥ˆ',
            'Bitcoin': 'â‚¿',
            'Ethereum': 'âŸ ',
            'Brent': 'ğŸ›¢ï¸',
            'Wti': 'ğŸ›¢ï¸',
            'Copper': 'ğŸ”¶',
            'Corn': 'ğŸŒ½',
            'Coffee': 'â˜•',
            'Wheat': 'ğŸŒ¾',
        }
        emoji = emoji_map.get(commodity, 'ğŸ“Š')

        message = (
            f"{emoji} **{commodity} å•†å“æ–°è** (ID: {news_id})\n"
            f"{'â”€' * 40}\n\n"
            f"{text}\n\n"
            f"{'â”€' * 40}\n"
            f"â° {time}"
        )

        return message
```

### æ¸¬è©¦è¨ˆç•«

#### 1. å–®å…ƒæ¸¬è©¦ï¼ˆé¸ç”¨ï¼‰

**æª”æ¡ˆ**ï¼š`tests/test_crawler/test_scheduler.py`

```python
"""
æ¸¬è©¦çˆ¬èŸ²èª¿åº¦å™¨
"""
import pytest
from src.crawler.config import CrawlerConfig
from src.crawler.scheduler import CrawlerScheduler


def test_format_news_message():
    """æ¸¬è©¦è¨Šæ¯æ ¼å¼åŒ–"""
    config = CrawlerConfig.from_env()
    scheduler = CrawlerScheduler(config)

    news = {
        'commodity': 'Gold',
        'news_id': 1,
        'text': 'Gold prices surge to new high',
        'time': '2026-01-02T14:30:00Z'
    }

    message = scheduler._format_news_message(news)

    assert 'Gold' in message
    assert 'ID: 1' in message
    assert 'Gold prices surge' in message
```

#### 2. æ•´åˆæ¸¬è©¦

**æ¸¬è©¦æ­¥é©Ÿ**ï¼š

1. **å•Ÿå‹• Bot**
   ```bash
   python scripts/run_bot.py
   ```

2. **é©—è­‰å•Ÿå‹•è¨Šæ¯**
   - Telegram ç¾¤çµ„æ”¶åˆ°ã€ŒChip House å‚³å‡ºå·¥ä½œè²...ã€è¨Šæ¯

3. **æ‰‹å‹•è§¸ç™¼çˆ¬å–**ï¼ˆè‹¥å¯¦ä½œäº† `/crawl_now` æŒ‡ä»¤ï¼‰
   - åœ¨ç¾¤çµ„ä¸­ç™¼é€ `/crawl_now`
   - é©—è­‰æ”¶åˆ°ã€Œæ­£åœ¨æ‰‹å‹•è§¸ç™¼...ã€è¨Šæ¯
   - é©—è­‰æ”¶åˆ°æ–°èé€šçŸ¥ï¼ˆè‹¥æœ‰æ–°æ–°èï¼‰

4. **ç­‰å¾…è‡ªå‹•çˆ¬å–**
   - ç­‰å¾… 5 åˆ†é˜
   - é©—è­‰çˆ¬èŸ²è‡ªå‹•åŸ·è¡Œ
   - é©—è­‰ Telegram é€šçŸ¥

5. **æª¢æŸ¥æª”æ¡ˆå„²å­˜**
   ```bash
   # æŸ¥çœ‹ä»Šå¤©çš„æ–°èæª”æ¡ˆ
   ls markets/Gold/
   cat markets/Gold/20260102.txt
   ```
   - é©—è­‰ ID æ ¼å¼æ­£ç¢ºï¼ˆ[1], [2], ...ï¼‰
   - é©—è­‰æ–°èå…§å®¹å®Œæ•´

6. **åœæ­¢ Bot**
   - Ctrl+C åœæ­¢
   - é©—è­‰æ—¥èªŒé¡¯ç¤ºã€Œçˆ¬èŸ²å®šæ™‚ä»»å‹™å·²åœæ­¢ã€

#### 3. å£“åŠ›æ¸¬è©¦ï¼ˆé¸ç”¨ï¼‰

**æ¸¬è©¦å ´æ™¯**ï¼š
- è¨­å®š `CRAWLER_INTERVAL_MINUTES=1`ï¼Œè§€å¯Ÿ 1 å°æ™‚
- é©—è­‰ç„¡è¨˜æ†¶é«”æ´©æ¼
- é©—è­‰æ—¥èªŒæª”æ¡ˆå¤§å°åˆç†

### æˆåŠŸæ¨™æº–

#### è‡ªå‹•åŒ–é©—è­‰
- [ ] Bot å¯æ­£å¸¸å•Ÿå‹•å’Œåœæ­¢
- [ ] çˆ¬èŸ²å®šæ™‚ä»»å‹™æ­£å¸¸é‹ä½œ
- [ ] ç„¡ Python ç•°å¸¸æˆ–éŒ¯èª¤æ—¥èªŒ

#### æ‰‹å‹•é©—è­‰
- [ ] Telegram é€šçŸ¥è¨Šæ¯æ ¼å¼ç¾è§€ã€å¯è®€æ€§å¼·
- [ ] `/crawl_now` æŒ‡ä»¤å¯æ­£å¸¸è§¸ç™¼çˆ¬å–ï¼ˆè‹¥å¯¦ä½œï¼‰
- [ ] æ–°èå„²å­˜åˆ°æª”æ¡ˆï¼Œæ ¼å¼æ­£ç¢º
- [ ] é‡è¤‡æ–°èä¸æœƒè¢«é‡è¤‡é€šçŸ¥
- [ ] çˆ¬èŸ²å¤±æ•—ä¸å½±éŸ¿ Bot å…¶ä»–åŠŸèƒ½ï¼ˆå¦‚ Claude å°è©±ï¼‰
- [ ] æ—¥èªŒè¨˜éŒ„è©³ç´°ä¸”æ˜“æ–¼é™¤éŒ¯

**å¯¦ä½œæç¤º**ï¼š
- æ­¤éšæ®µè‘—é‡æ–¼ä½¿ç”¨è€…é«”é©—å’Œç©©å®šæ€§
- å»ºè­°åœ¨æ¸¬è©¦ç¾¤çµ„ä¸­é€²è¡Œå®Œæ•´æ¸¬è©¦
- é ä¼°æ™‚é–“ï¼š1-1.5 å°æ™‚

---

## éšæ®µå…­ï¼šæ–‡æª”èˆ‡æ”¶å°¾

### æ¦‚è¿°

æ’°å¯«ä½¿ç”¨èªªæ˜ã€æ›´æ–°å°ˆæ¡ˆæ–‡æª”ã€è™•ç†é‚Šç•Œæƒ…æ³ã€å„ªåŒ–éŒ¯èª¤è™•ç†ã€‚

### éœ€è¦å‰µå»º/ä¿®æ”¹çš„æª”æ¡ˆ

#### 1. æ›´æ–°å°ˆæ¡ˆ READMEï¼ˆé¸ç”¨ï¼‰

**æª”æ¡ˆ**ï¼š`README.md`

**æ–°å¢å…§å®¹**ï¼ˆåœ¨é©ç•¶ä½ç½®ï¼‰ï¼š

```markdown
## å•†å“æ–°èçˆ¬èŸ²åŠŸèƒ½

Bot å…§å»ºå•†å“æ–°èçˆ¬èŸ²ï¼Œå¯è‡ªå‹•å¾ tradingeconomics.com æŠ“å–å•†å“ç›¸é—œæ–°èã€‚

### åŠŸèƒ½ç‰¹è‰²

- â° æ¯ 5 åˆ†é˜è‡ªå‹•çˆ¬å–ä¸€æ¬¡ï¼ˆå¯é…ç½®ï¼‰
- ğŸ“ æ–°èä¿å­˜åˆ° `markets/<å•†å“>/yyyymmdd.txt`
- ğŸ“± å³æ™‚ç™¼é€åˆ° Telegram ç¾¤çµ„
- ğŸ”’ é˜²çˆ¬èŸ²ç­–ç•¥ï¼ˆå»¶é²ã€User-Agent è¼ªæ›ï¼‰
- ğŸ”„ è‡ªå‹•å»é‡

### é…ç½®

åœ¨ `.env` æª”æ¡ˆä¸­é…ç½®ï¼š

```bash
# å•Ÿç”¨çˆ¬èŸ²
CRAWLER_ENABLED=true

# çˆ¬å–é–“éš”ï¼ˆåˆ†é˜ï¼‰
CRAWLER_INTERVAL_MINUTES=5

# é€šçŸ¥ç¾¤çµ„ ID
CRAWLER_NOTIFY_GROUPS=-1001234567890
```

### æ‰‹å‹•è§¸ç™¼

ç¾¤çµ„ç®¡ç†å“¡å¯ä½¿ç”¨ `/crawl_now` æŒ‡ä»¤æ‰‹å‹•è§¸ç™¼ä¸€æ¬¡çˆ¬å–ã€‚

### åœç”¨çˆ¬èŸ²

è‹¥è¦åœç”¨çˆ¬èŸ²åŠŸèƒ½ï¼š

```bash
CRAWLER_ENABLED=false
```
```

#### 2. å»ºç«‹çˆ¬èŸ²æ¨¡çµ„ README

**æª”æ¡ˆ**ï¼š`src/crawler/README.md`

```markdown
# å•†å“æ–°èçˆ¬èŸ²æ¨¡çµ„

## æ¶æ§‹æ¦‚è¿°

```
src/crawler/
â”œâ”€â”€ __init__.py             # æ¨¡çµ„åˆå§‹åŒ–
â”œâ”€â”€ config.py               # é…ç½®ç®¡ç†
â”œâ”€â”€ commodity_mapper.py     # å•†å“åç¨±æ˜ å°„
â”œâ”€â”€ news_storage.py         # æ–°èå„²å­˜ç®¡ç†
â”œâ”€â”€ news_crawler.py         # çˆ¬èŸ²æ ¸å¿ƒ
â””â”€â”€ scheduler.py            # å®šæ™‚ä»»å‹™èª¿åº¦
```

## æ¨¡çµ„è·è²¬

### config.py - é…ç½®ç®¡ç†
- å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥çˆ¬èŸ²é…ç½®
- æä¾›é…ç½®é©—è­‰

### commodity_mapper.py - å•†å“åç¨±æ˜ å°„
- ç¶­è­·æ–°èé—œéµå­—èˆ‡ markets ç›®éŒ„çš„æ˜ å°„è¡¨
- å¾æ–°èæ–‡æœ¬ä¸­æå–å•†å“åç¨±

### news_storage.py - æ–°èå„²å­˜ç®¡ç†
- å°‡æ–°èä¿å­˜åˆ° `markets/<å•†å“>/yyyymmdd.txt`
- è‡ªå‹•ç®¡ç†éå¢ ID
- æª¢æŸ¥æ–°èé‡è¤‡

### news_crawler.py - çˆ¬èŸ²æ ¸å¿ƒ
- å¾ tradingeconomics.com æŠ“å– HTML
- è§£ææ–°èåˆ—è¡¨
- æå–å•†å“ä¸¦å„²å­˜

### scheduler.py - å®šæ™‚ä»»å‹™èª¿åº¦
- ä½¿ç”¨ APScheduler ç®¡ç†å®šæ™‚ä»»å‹™
- æ•´åˆ Telegram é€šçŸ¥
- èˆ‡ Bot ç”Ÿå‘½é€±æœŸåŒæ­¥

## ä½¿ç”¨ç¯„ä¾‹

### ç¨ç«‹ä½¿ç”¨çˆ¬èŸ²

```python
import asyncio
from src.crawler.config import CrawlerConfig
from src.crawler.news_crawler import NewsCrawler

async def crawl():
    config = CrawlerConfig.from_env()
    crawler = NewsCrawler(config)
    saved_news = await crawler.crawl()
    print(f"å…±ä¿å­˜ {len(saved_news)} å‰‡æ–°è")

asyncio.run(crawl())
```

### æ•´åˆåˆ° Bot

çˆ¬èŸ²å·²è‡ªå‹•æ•´åˆåˆ° `TelegramBot` çš„ç”Ÿå‘½é€±æœŸä¸­ï¼š

- Bot å•Ÿå‹•æ™‚è‡ªå‹•å•Ÿå‹•çˆ¬èŸ²å®šæ™‚ä»»å‹™
- Bot é—œé–‰æ™‚è‡ªå‹•åœæ­¢çˆ¬èŸ²

## ç¶­è­·æŒ‡å—

### æ›´æ–°å•†å“æ˜ å°„è¡¨

ç·¨è¼¯ `commodity_mapper.py` çš„ `COMMODITY_MAP`ï¼š

```python
COMMODITY_MAP = {
    'new_commodity': 'NewCommodity',  # æ–°å¢
    # ...
}
```

### èª¿æ•´ HTML é¸æ“‡å™¨

è‹¥ç¶²ç«™çµæ§‹æ”¹è®Šï¼Œç·¨è¼¯ `news_crawler.py` çš„ `parse_news()` æ–¹æ³•ã€‚

### èª¿æ•´çˆ¬å–é–“éš”

ä¿®æ”¹ `.env` æª”æ¡ˆï¼š

```bash
CRAWLER_INTERVAL_MINUTES=10  # æ”¹ç‚º 10 åˆ†é˜
```

## æ•…éšœæ’é™¤

### å•é¡Œï¼šçˆ¬èŸ²ç„¡æ³•è§£ææ–°è

**åŸå› **ï¼šç¶²ç«™ HTML çµæ§‹æ”¹è®Š

**è§£æ±º**ï¼š
1. è¨ªå•ç›®æ¨™ç¶²ç«™ï¼Œæª¢æŸ¥ HTML çµæ§‹
2. ä½¿ç”¨ç€è¦½å™¨é–‹ç™¼è€…å·¥å…·æ‰¾åˆ°æ–°çš„é¸æ“‡å™¨
3. æ›´æ–° `news_crawler.py` çš„ CSS é¸æ“‡å™¨

### å•é¡Œï¼šTelegram é€šçŸ¥å¤±æ•—

**åŸå› **ï¼šç¾¤çµ„ ID éŒ¯èª¤æˆ– Bot ç„¡æ¬Šé™

**è§£æ±º**ï¼š
1. ç¢ºèª `CRAWLER_NOTIFY_GROUPS` è¨­å®šæ­£ç¢º
2. ç¢ºèª Bot å·²åŠ å…¥ç¾¤çµ„
3. ç¢ºèª Bot æœ‰ç™¼é€è¨Šæ¯æ¬Šé™

### å•é¡Œï¼šæ–°èé‡è¤‡ä¿å­˜

**åŸå› **ï¼šå»é‡é‚è¼¯å¤±æ•ˆ

**è§£æ±º**ï¼š
1. æª¢æŸ¥ `news_storage.py` çš„ `check_duplicate()` æ–¹æ³•
2. è€ƒæ…®ä½¿ç”¨ hash å»é‡ï¼ˆæ›´å¯é ï¼‰
```

#### 3. å»ºç«‹æ•…éšœæ’é™¤æ–‡æª”

**æª”æ¡ˆ**ï¼š`docs/crawler-troubleshooting.md`

```markdown
# çˆ¬èŸ²æ•…éšœæ’é™¤æŒ‡å—

## å¸¸è¦‹å•é¡Œ

### 1. çˆ¬èŸ²ç„¡æ³•å•Ÿå‹•

**ç—‡ç‹€**ï¼šæ—¥èªŒé¡¯ç¤ºã€Œçˆ¬èŸ²å·²åœç”¨ã€

**è§£æ±º**ï¼š
```bash
# æª¢æŸ¥ .env æª”æ¡ˆ
CRAWLER_ENABLED=true
```

### 2. ç¶²é æŠ“å–å¤±æ•—ï¼ˆHTTP 403/429ï¼‰

**ç—‡ç‹€**ï¼šæ—¥èªŒé¡¯ç¤ºã€ŒHTTP 403ã€æˆ–ã€ŒHTTP 429ã€

**åŸå› **ï¼šIP è¢«å°é–æˆ–è«‹æ±‚éæ–¼é »ç¹

**è§£æ±º**ï¼š
- å¢åŠ çˆ¬å–é–“éš”ï¼š`CRAWLER_INTERVAL_MINUTES=10`
- æª¢æŸ¥ User-Agent æ˜¯å¦æ­£å¸¸è¼ªæ›
- è€ƒæ…®ä½¿ç”¨ä»£ç† IPï¼ˆé€²éšï¼‰

### 3. ç„¡æ³•è§£ææ–°è

**ç—‡ç‹€**ï¼šæ—¥èªŒé¡¯ç¤ºã€ŒæˆåŠŸè§£æ 0 å‰‡æ–°èã€

**åŸå› **ï¼šHTML çµæ§‹æ”¹è®Š

**è§£æ±º**ï¼š
1. è¨ªå• https://tradingeconomics.com/stream?c=commodity
2. æŒ‰ F12 æ‰“é–‹é–‹ç™¼è€…å·¥å…·
3. æ‰¾åˆ°æ–°èå®¹å™¨çš„ CSS é¸æ“‡å™¨
4. æ›´æ–° `src/crawler/news_crawler.py` çš„ `parse_news()` æ–¹æ³•

### 4. Telegram é€šçŸ¥å¤±æ•—

**ç—‡ç‹€**ï¼šæ—¥èªŒé¡¯ç¤ºã€Œç™¼é€é€šçŸ¥å¤±æ•—ã€

**å¯èƒ½åŸå› **ï¼š
- ç¾¤çµ„ ID éŒ¯èª¤
- Bot æœªåŠ å…¥ç¾¤çµ„
- Bot ç„¡ç™¼é€è¨Šæ¯æ¬Šé™
- è¨Šæ¯éé•·ï¼ˆè¶…é 4096 å­—å…ƒï¼‰

**è§£æ±º**ï¼š
- æª¢æŸ¥ `CRAWLER_NOTIFY_GROUPS` è¨­å®š
- ç¢ºèª Bot åœ¨ç¾¤çµ„ä¸­
- æª¢æŸ¥è¨Šæ¯é•·åº¦é™åˆ¶é‚è¼¯

### 5. æª”æ¡ˆå¯«å…¥å¤±æ•—

**ç—‡ç‹€**ï¼šæ—¥èªŒé¡¯ç¤ºã€Œä¿å­˜æ–°èå¤±æ•—ã€

**å¯èƒ½åŸå› **ï¼š
- `markets/` ç›®éŒ„æ¬Šé™ä¸è¶³
- ç£ç¢Ÿç©ºé–“ä¸è¶³
- è·¯å¾‘éŒ¯èª¤

**è§£æ±º**ï¼š
```bash
# æª¢æŸ¥ç›®éŒ„æ¬Šé™
ls -la markets/

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
mkdir -p markets/Gold markets/Silver
```

## é™¤éŒ¯æŠ€å·§

### å•Ÿç”¨è©³ç´°æ—¥èªŒ

```bash
# .env
DEBUG=true
```

### æª¢è¦–çˆ¬èŸ²æ—¥èªŒ

```bash
# æŸ¥çœ‹ä»Šå¤©çš„æ—¥èªŒ
cat logs/bot_2026-01-02.log | grep crawler

# å³æ™‚æŸ¥çœ‹
tail -f logs/bot_2026-01-02.log
```

### æ‰‹å‹•æ¸¬è©¦çˆ¬èŸ²

```python
# test_manual_crawl.py
import asyncio
from src.crawler.config import CrawlerConfig
from src.crawler.news_crawler import NewsCrawler

async def test():
    config = CrawlerConfig.from_env()
    crawler = NewsCrawler(config)

    # æ¸¬è©¦æŠ“å–
    html = await crawler.fetch_page()
    print(f"HTML é•·åº¦: {len(html)}")

    # æ¸¬è©¦è§£æ
    news_list = crawler.parse_news(html)
    print(f"è§£æåˆ° {len(news_list)} å‰‡æ–°è")
    for news in news_list[:3]:
        print(f"  - {news['title']}")

asyncio.run(test())
```

## æ•ˆèƒ½ç›£æ§

### è¨˜æ†¶é«”ä½¿ç”¨

```bash
# ç›£æ§ Python ç¨‹åºè¨˜æ†¶é«”
ps aux | grep python
```

### çˆ¬å–æˆåŠŸç‡

å®šæœŸæª¢æŸ¥æ—¥èªŒï¼Œçµ±è¨ˆæˆåŠŸ/å¤±æ•—æ¬¡æ•¸ï¼š

```bash
grep "çˆ¬å–å®Œæˆ" logs/bot_*.log | wc -l  # æˆåŠŸæ¬¡æ•¸
grep "çˆ¬å–å¤±æ•—" logs/bot_*.log | wc -l  # å¤±æ•—æ¬¡æ•¸
```
```

### æˆåŠŸæ¨™æº–

#### è‡ªå‹•åŒ–é©—è­‰
- [ ] æ–‡æª”æª”æ¡ˆå­˜åœ¨ä¸”æ ¼å¼æ­£ç¢º
- [ ] README åŒ…å«çˆ¬èŸ²åŠŸèƒ½èªªæ˜

#### æ‰‹å‹•é©—è­‰
- [ ] æ–‡æª”æ¸…æ™°æ˜“æ‡‚ï¼Œæœ‰ç¯„ä¾‹ç¨‹å¼ç¢¼
- [ ] æ•…éšœæ’é™¤æŒ‡å—æ¶µè“‹å¸¸è¦‹å•é¡Œ
- [ ] ç¶­è­·æŒ‡å—æä¾›å¿…è¦çš„æ“ä½œæ­¥é©Ÿ

**å¯¦ä½œæç¤º**ï¼š
- æ–‡æª”æ‡‰é¢å‘æœªä¾†çš„ç¶­è­·è€…
- ç¯„ä¾‹ç¨‹å¼ç¢¼æ‡‰å¯ç›´æ¥åŸ·è¡Œ
- é ä¼°æ™‚é–“ï¼š30-45 åˆ†é˜

---

## æ•´é«”æ¸¬è©¦èˆ‡é©—æ”¶

### å®Œæ•´ç«¯åˆ°ç«¯æ¸¬è©¦

1. **ç’°å¢ƒæº–å‚™**
   ```bash
   # ç¢ºèªç’°å¢ƒè®Šæ•¸
   cat .env | grep CRAWLER

   # ç¢ºèªä¾è³´å·²å®‰è£
   pip list | grep -E "httpx|beautifulsoup4|lxml|APScheduler"
   ```

2. **å•Ÿå‹• Bot**
   ```bash
   python scripts/run_bot.py
   ```

3. **é©—è­‰çˆ¬èŸ²å•Ÿå‹•**
   - [ ] æ—¥èªŒé¡¯ç¤ºã€Œçˆ¬èŸ²å®šæ™‚ä»»å‹™å·²å•Ÿå‹•ã€
   - [ ] æ—¥èªŒé¡¯ç¤ºçˆ¬å–é–“éš”ï¼ˆå¦‚ã€Œæ¯ 5 åˆ†é˜ (Â±15 ç§’) åŸ·è¡Œä¸€æ¬¡ã€ï¼‰

4. **ç­‰å¾…ç¬¬ä¸€æ¬¡çˆ¬å–**
   - [ ] ç´„ 5 åˆ†é˜å¾Œï¼Œæ—¥èªŒé¡¯ç¤ºã€Œé–‹å§‹çˆ¬å–å•†å“æ–°èã€
   - [ ] æ—¥èªŒé¡¯ç¤ºã€Œç¶²é æŠ“å–æˆåŠŸã€
   - [ ] æ—¥èªŒé¡¯ç¤ºã€ŒæˆåŠŸè§£æ X å‰‡æ–°èã€
   - [ ] æ—¥èªŒé¡¯ç¤ºã€Œæ–°èå·²ä¿å­˜ï¼šGold ID=1ã€ï¼ˆæˆ–å…¶ä»–å•†å“ï¼‰
   - [ ] æ—¥èªŒé¡¯ç¤ºã€Œå·²ç™¼é€é€šçŸ¥åˆ°ç¾¤çµ„ã€

5. **æª¢æŸ¥ Telegram ç¾¤çµ„**
   - [ ] æ”¶åˆ°æ–°èé€šçŸ¥è¨Šæ¯
   - [ ] è¨Šæ¯æ ¼å¼æ­£ç¢ºï¼ˆå•†å“åã€IDã€å…§å®¹ã€æ™‚é–“ï¼‰
   - [ ] è¡¨æƒ…ç¬¦è™Ÿé¡¯ç¤ºæ­£ç¢º

6. **æª¢æŸ¥æª”æ¡ˆå„²å­˜**
   ```bash
   # æŸ¥çœ‹ä»Šå¤©çš„æ–°è
   ls markets/Gold/
   cat markets/Gold/20260102.txt
   ```
   - [ ] æª”æ¡ˆå­˜åœ¨
   - [ ] ID æ ¼å¼æ­£ç¢ºï¼š`[1] æ–°èå…§å®¹`
   - [ ] åˆ†éš”ç·šæ­£ç¢ºï¼š`--------...`

7. **æ¸¬è©¦æ‰‹å‹•è§¸ç™¼**ï¼ˆè‹¥å¯¦ä½œï¼‰
   - [ ] åœ¨ç¾¤çµ„ä¸­ç™¼é€ `/crawl_now`
   - [ ] æ”¶åˆ°ã€Œæ­£åœ¨æ‰‹å‹•è§¸ç™¼...ã€å›è¦†
   - [ ] çˆ¬èŸ²ç«‹å³åŸ·è¡Œ
   - [ ] æ”¶åˆ°æ–°æ–°èé€šçŸ¥ï¼ˆè‹¥æœ‰ï¼‰

8. **æ¸¬è©¦å»é‡**
   - [ ] ç­‰å¾…ä¸‹ä¸€æ¬¡çˆ¬å–ï¼ˆ5 åˆ†é˜å¾Œï¼‰
   - [ ] é‡è¤‡æ–°èä¸å†ç™¼é€é€šçŸ¥
   - [ ] æ—¥èªŒé¡¯ç¤ºã€Œæ–°èé‡è¤‡ï¼Œå¿½ç•¥ã€

9. **æ¸¬è©¦åœç”¨åŠŸèƒ½**
   ```bash
   # ä¿®æ”¹ .env
   CRAWLER_ENABLED=false

   # é‡å•Ÿ Bot
   python scripts/run_bot.py
   ```
   - [ ] æ—¥èªŒé¡¯ç¤ºã€Œçˆ¬èŸ²å·²åœç”¨ï¼Œä¸å•Ÿå‹•å®šæ™‚ä»»å‹™ã€
   - [ ] ç„¡çˆ¬å–æ´»å‹•

10. **å£“åŠ›æ¸¬è©¦**ï¼ˆé¸ç”¨ï¼‰
    ```bash
    # è¨­å®šç‚º 1 åˆ†é˜é–“éš”
    CRAWLER_INTERVAL_MINUTES=1
    ```
    - [ ] é‹è¡Œ 1 å°æ™‚ï¼Œç„¡ç•°å¸¸
    - [ ] è¨˜æ†¶é«”ç©©å®š
    - [ ] æ—¥èªŒæª”æ¡ˆå¤§å°åˆç†

### é‚Šç•Œæƒ…æ³æ¸¬è©¦

1. **ç¶²è·¯æ•…éšœ**
   - [ ] æ‹”é™¤ç¶²è·¯ç·šï¼Œçˆ¬èŸ²å¤±æ•—ä½†ä¸å½±éŸ¿ Bot
   - [ ] æ—¥èªŒé¡¯ç¤ºã€Œç¶²é æŠ“å–ç•°å¸¸ã€
   - [ ] é‡æ–°é€£æ¥å¾Œï¼Œä¸‹æ¬¡çˆ¬å–æ¢å¾©æ­£å¸¸

2. **ç›®æ¨™ç¶²ç«™ç„¡å›æ‡‰**
   - [ ] ä¿®æ”¹ `CRAWLER_TARGET_URL` ç‚ºç„¡æ•ˆ URL
   - [ ] çˆ¬èŸ²å¤±æ•—ï¼Œæ—¥èªŒè¨˜éŒ„éŒ¯èª¤
   - [ ] Bot å…¶ä»–åŠŸèƒ½æ­£å¸¸

3. **ç£ç¢Ÿç©ºé–“ä¸è¶³**ï¼ˆæ¨¡æ“¬ï¼‰
   - [ ] çˆ¬èŸ²å ±éŒ¯ä½†ä¸å´©æ½°
   - [ ] æ—¥èªŒè¨˜éŒ„ã€Œä¿å­˜æ–°èå¤±æ•—ã€

4. **Telegram API é™æµ**
   - [ ] çŸ­æ™‚é–“å…§ç™¼é€å¤§é‡è¨Šæ¯
   - [ ] å¤±æ•—è¨Šæ¯æœ‰æ—¥èªŒè¨˜éŒ„
   - [ ] ä¸å½±éŸ¿å¾ŒçºŒçˆ¬å–

## æ½›åœ¨é¢¨éšªèˆ‡è§£æ±ºæ–¹æ¡ˆ

### é¢¨éšª 1ï¼šç¶²ç«™ HTML çµæ§‹æ”¹è®Š

**å½±éŸ¿**ï¼šçˆ¬èŸ²ç„¡æ³•è§£ææ–°è

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨å¤šå€‹å‚™ç”¨ CSS é¸æ“‡å™¨
- å®šæœŸç›£æ§çˆ¬å–æˆåŠŸç‡
- å»ºç«‹å‘Šè­¦æ©Ÿåˆ¶ï¼ˆå¦‚çˆ¬å–å¤±æ•—è¶…é 3 æ¬¡ç™¼é€é€šçŸ¥ï¼‰

**ç¨‹å¼ç¢¼ç¯„ä¾‹**ï¼ˆå·²åœ¨ `news_crawler.py` å¯¦ä½œï¼‰ï¼š

```python
# ä¸»è¦é¸æ“‡å™¨
items = soup.select('div.stream-item')

if not items:
    # å‚™ç”¨é¸æ“‡å™¨
    items = soup.select('article.news-item')

if not items:
    logger.warning("æ‰€æœ‰é¸æ“‡å™¨éƒ½ç„¡æ³•åŒ¹é…")
```

### é¢¨éšª 2ï¼šIP è¢«å°é–

**å½±éŸ¿**ï¼šç„¡æ³•æŠ“å–ç¶²é ï¼ˆHTTP 403/429ï¼‰

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- å·²å¯¦ä½œï¼šéš¨æ©Ÿå»¶é²ã€User-Agent è¼ªæ›ã€Headers å½è£
- é€²éšï¼šæ•´åˆä»£ç† IP æ± ï¼ˆæœ¬æ¬¡å¯¦ä½œä¸åŒ…å«ï¼‰
- æ‡‰æ€¥ï¼šå¢åŠ çˆ¬å–é–“éš”ï¼ˆå¦‚ 10 åˆ†é˜ï¼‰

### é¢¨éšª 3ï¼šå•†å“åç¨±åŒ¹é…ä¸æº–

**å½±éŸ¿**ï¼šæ–°èä¿å­˜åˆ°éŒ¯èª¤å•†å“æˆ–æœªä¿å­˜

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- æ“´å…… `COMMODITY_MAP` æ˜ å°„è¡¨
- è¨˜éŒ„æœªåŒ¹é…çš„æ–°èï¼ˆæ—¥èªŒç´šåˆ¥ DEBUGï¼‰
- å®šæœŸäººå·¥æª¢è¦–ï¼Œå„ªåŒ–æ˜ å°„è¦å‰‡

**æ”¹é€²ç¯„ä¾‹**ï¼ˆæœªä¾†å¯å¯¦ä½œï¼‰ï¼š

```python
# ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æé«˜æº–ç¢ºæ€§
import re

COMMODITY_PATTERNS = {
    'Gold': [r'\bgold\b', r'\bxau\b', r'\bgc\b'],
    'Silver': [r'\bsilver\b', r'\bxag\b', r'\bsi\b'],
}
```

### é¢¨éšª 4ï¼šæ–°èé‡è¤‡æª¢æ¸¬å¤±æ•ˆ

**å½±éŸ¿**ï¼šåŒä¸€æ–°èè¢«å¤šæ¬¡ä¿å­˜

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- ç›®å‰ä½¿ç”¨ç°¡å–®å­—ä¸²åŒ…å«æª¢æŸ¥ï¼ˆå¯æ‡‰å°å¤§éƒ¨åˆ†æƒ…æ³ï¼‰
- é€²éšï¼šä½¿ç”¨ MD5 hash å»é‡ï¼ˆæ›´å¯é ï¼‰

**æ”¹é€²ç¯„ä¾‹**ï¼ˆæœªä¾†å¯å¯¦ä½œï¼‰ï¼š

```python
import hashlib

def get_news_hash(text: str) -> str:
    return hashlib.md5(text.encode('utf-8')).hexdigest()

# å„²å­˜ hash åˆ°æª”æ¡ˆæˆ–è³‡æ–™åº«ï¼Œæª¢æŸ¥æ™‚æ¯”å° hash
```

### é¢¨éšª 5ï¼šæª”æ¡ˆä½µç™¼å¯«å…¥è¡çª

**å½±éŸ¿**ï¼šå¤šå€‹çˆ¬èŸ²å¯¦ä¾‹åŒæ™‚å¯«å…¥ï¼Œè³‡æ–™æå£

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- å·²å¯¦ä½œï¼šæª”æ¡ˆé–ï¼ˆLinux/Unix ä½¿ç”¨ fcntlï¼‰
- Windowsï¼šç›®å‰ä½¿ç”¨ try-except å¿½ç•¥ï¼ˆå–®å¯¦ä¾‹é‹è¡Œç„¡å•é¡Œï¼‰
- ç¢ºä¿åªé‹è¡Œä¸€å€‹ Bot å¯¦ä¾‹

**Windows æª”æ¡ˆé–æ”¹é€²**ï¼ˆæœªä¾†å¯å¯¦ä½œï¼‰ï¼š

```python
import msvcrt

def save_with_windows_lock(file_path, content):
    with open(file_path, 'a', encoding='utf-8') as f:
        msvcrt.locking(f.fileno(), msvcrt.LK_LOCK, 1024)
        f.write(content)
        msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1024)
```

## å¾ŒçºŒå„ªåŒ–å»ºè­°

å®ŒæˆåŸºæœ¬å¯¦ä½œå¾Œï¼Œå¯è€ƒæ…®ä»¥ä¸‹å„ªåŒ–ï¼ˆä¸åœ¨æœ¬æ¬¡å¯¦ä½œç¯„åœï¼‰ï¼š

### 1. è³‡æ–™åº«å„²å­˜

**ç›®æ¨™**ï¼šä½¿ç”¨ SQLite æ›¿ä»£ç´”æ–‡å­—æª”æ¡ˆ

**å„ªé»**ï¼š
- æ›´å¯é çš„å»é‡æ©Ÿåˆ¶
- æ”¯æ´è¤‡é›œæŸ¥è©¢
- æ›´å¥½çš„ä½µç™¼æ§åˆ¶

**å¯¦ä½œæ¦‚è¦**ï¼š
```python
# è³‡æ–™è¡¨çµæ§‹
CREATE TABLE news (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    commodity TEXT NOT NULL,
    content TEXT NOT NULL,
    content_hash TEXT UNIQUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
```

### 2. å¤šèªè¨€æ”¯æ´

**ç›®æ¨™**ï¼šç¿»è­¯è‹±æ–‡æ–°èç‚ºç¹é«”ä¸­æ–‡

**å¯¦ä½œæ–¹å¼**ï¼š
- ä½¿ç”¨ç¿»è­¯ APIï¼ˆGoogle Translate, DeepLï¼‰
- æˆ–æ•´åˆ Claude API é€²è¡Œç¿»è­¯

### 3. æƒ…æ„Ÿåˆ†æ

**ç›®æ¨™**ï¼šåˆ†ææ–°èæƒ…æ„Ÿï¼ˆçœ‹æ¼²/çœ‹è·Œï¼‰

**å¯¦ä½œæ–¹å¼**ï¼š
- ä½¿ç”¨ NLP åº«ï¼ˆå¦‚ TextBlob, VADERï¼‰
- æˆ–ä½¿ç”¨ Claude API é€²è¡Œåˆ†æ

### 4. Web Dashboard

**ç›®æ¨™**ï¼šæä¾›ç¶²é ä»‹é¢æŸ¥çœ‹æ­·å²æ–°è

**æŠ€è¡“æ£§**ï¼š
- å¾Œç«¯ï¼šFlask/FastAPI
- å‰ç«¯ï¼šVue.js/React
- è³‡æ–™åº«ï¼šSQLite/PostgreSQL

### 5. å¤šä¾†æºæ•´åˆ

**ç›®æ¨™**ï¼šæ•´åˆå¤šå€‹æ–°èä¾†æº

**ä¾†æºå»ºè­°**ï¼š
- Reuters
- Bloomberg
- Investing.com
- MarketWatch

### 6. å‘Šè­¦æ©Ÿåˆ¶

**ç›®æ¨™**ï¼šçˆ¬å–å¤±æ•—æ™‚ç™¼é€å‘Šè­¦

**å¯¦ä½œæ–¹å¼**ï¼š
```python
# é€£çºŒå¤±æ•— 3 æ¬¡æ™‚ç™¼é€å‘Šè­¦
if consecutive_failures >= 3:
    await send_alert("âš ï¸ çˆ¬èŸ²é€£çºŒå¤±æ•— 3 æ¬¡ï¼Œè«‹æª¢æŸ¥")
```

## æ™‚é–“é ä¼°ç¸½çµ

| éšæ®µ | é ä¼°æ™‚é–“ | ç´¯è¨ˆæ™‚é–“ |
|------|---------|---------|
| éšæ®µä¸€ï¼šå‰ç½®æº–å‚™èˆ‡ç’°å¢ƒè¨­å®š | 15-30 åˆ†é˜ | 0.5 å°æ™‚ |
| éšæ®µäºŒï¼šåŸºç¤æ¶æ§‹æ¨¡çµ„ | 1-1.5 å°æ™‚ | 2 å°æ™‚ |
| éšæ®µä¸‰ï¼šçˆ¬èŸ²æ ¸å¿ƒæ¨¡çµ„ | 1.5-2 å°æ™‚ | 4 å°æ™‚ |
| éšæ®µå››ï¼šå®šæ™‚ä»»å‹™æ•´åˆ | 30-45 åˆ†é˜ | 4.75 å°æ™‚ |
| éšæ®µäº”ï¼šTelegram é€šçŸ¥å„ªåŒ–èˆ‡æ¸¬è©¦ | 1-1.5 å°æ™‚ | 6 å°æ™‚ |
| éšæ®µå…­ï¼šæ–‡æª”èˆ‡æ”¶å°¾ | 30-45 åˆ†é˜ | 6.5-7 å°æ™‚ |
| **ç¸½è¨ˆ** | **5.5-7 å°æ™‚** | - |

**æ³¨æ„**ï¼šå¯¦éš›æ™‚é–“å¯èƒ½å› ä»¥ä¸‹å› ç´ è®ŠåŒ–ï¼š
- HTML çµæ§‹åˆ†æçš„è¤‡é›œåº¦
- é™¤éŒ¯å’Œèª¿æ•´çš„æ™‚é–“
- æ¸¬è©¦çš„å®Œæ•´ç¨‹åº¦
- å°å°ˆæ¡ˆæ¶æ§‹çš„ç†Ÿæ‚‰åº¦

## æœ€çµ‚é©—æ”¶æ¸…å–®

### åŠŸèƒ½æ€§

- [ ] çˆ¬èŸ²å¯æ­£å¸¸å•Ÿå‹•å’Œåœæ­¢
- [ ] å®šæ™‚ä»»å‹™æŒ‰é…ç½®é–“éš”åŸ·è¡Œ
- [ ] ç¶²é æŠ“å–æˆåŠŸï¼ˆHTTP 200ï¼‰
- [ ] HTML è§£ææ­£ç¢ºï¼Œèƒ½æå–æ–°è
- [ ] å•†å“åç¨±åŒ¹é…æº–ç¢º
- [ ] æ–°èæ­£ç¢ºå„²å­˜åˆ° `markets/<å•†å“>/yyyymmdd.txt`
- [ ] ID éå¢æ­£å¸¸ï¼ˆ[1], [2], [3]...ï¼‰
- [ ] é‡è¤‡æ–°èè¢«éæ¿¾
- [ ] Telegram é€šçŸ¥æ­£å¸¸ç™¼é€
- [ ] è¨Šæ¯æ ¼å¼æ¸…æ™°ç¾è§€
- [ ] æ‰‹å‹•è§¸ç™¼æŒ‡ä»¤å¯ç”¨ï¼ˆè‹¥å¯¦ä½œï¼‰
- [ ] åœç”¨åŠŸèƒ½æ­£å¸¸ï¼ˆCRAWLER_ENABLED=falseï¼‰

### éåŠŸèƒ½æ€§

- [ ] é˜²çˆ¬èŸ²ç­–ç•¥ç”Ÿæ•ˆï¼ˆå»¶é²ã€UA è¼ªæ›ï¼‰
- [ ] éŒ¯èª¤è™•ç†å®Œå–„ï¼Œä¸å´©æ½°
- [ ] æ—¥èªŒè¨˜éŒ„è©³ç´°ä¸”æœ‰ç”¨
- [ ] è¨˜æ†¶é«”ä½¿ç”¨ç©©å®šï¼ˆç„¡æ´©æ¼ï¼‰
- [ ] ä¸å½±éŸ¿ Bot å…¶ä»–åŠŸèƒ½
- [ ] ç¨‹å¼ç¢¼çµæ§‹æ¸…æ™°ï¼Œæ˜“æ–¼ç¶­è­·
- [ ] ç’°å¢ƒè®Šæ•¸é…ç½®éˆæ´»

### æ–‡æª”

- [ ] README åŒ…å«çˆ¬èŸ²åŠŸèƒ½èªªæ˜
- [ ] çˆ¬èŸ²æ¨¡çµ„æœ‰ç¨ç«‹ README
- [ ] æ•…éšœæ’é™¤æ–‡æª”å®Œæ•´
- [ ] ç¨‹å¼ç¢¼è¨»è§£æ¸…æ™°
- [ ] æœ‰ä½¿ç”¨ç¯„ä¾‹

## åƒè€ƒè³‡æº

### æŠ€è¡“æ–‡æª”

- **httpx**: https://www.python-httpx.org/
- **BeautifulSoup4**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **APScheduler**: https://apscheduler.readthedocs.io/
- **python-telegram-bot**: https://docs.python-telegram-bot.org/

### ç›¸é—œç ”ç©¶

- ç ”ç©¶å ±å‘Šï¼š`thoughts/shared/research/2026-01-02-commodity-news-crawler-research.md`

### CSS é¸æ“‡å™¨åƒè€ƒ

- **MDN CSS Selectors**: https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors
- **BeautifulSoup CSS Selectors**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors

---

**è¨ˆç•«åˆ¶å®šæ—¥æœŸ**ï¼š2026-01-02
**é ä¼°ç¸½æ™‚é–“**ï¼š5.5-7 å°æ™‚
**å„ªå…ˆç´š**ï¼šğŸ”´ é«˜ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
**è² è²¬äºº**ï¼šé–‹ç™¼åœ˜éšŠ

---

## é™„éŒ„ï¼šå¿«é€Ÿå•Ÿå‹•æŒ‡ä»¤

```bash
# 1. å®‰è£ä¾è³´
pip install httpx beautifulsoup4 lxml APScheduler

# 2. é…ç½®ç’°å¢ƒè®Šæ•¸ï¼ˆç·¨è¼¯ .envï¼‰
nano .env
# æ–°å¢ï¼š
# CRAWLER_ENABLED=true
# CRAWLER_NOTIFY_GROUPS=-1001234567890

# 3. å•Ÿå‹• Bot
python scripts/run_bot.py

# 4. æª¢è¦–æ—¥èªŒ
tail -f logs/bot_$(date +%Y-%m-%d).log

# 5. æª¢æŸ¥æ–°èæª”æ¡ˆ
ls markets/Gold/
cat markets/Gold/$(date +%Y%m%d).txt
```

ç¥å¯¦ä½œé †åˆ©ï¼ğŸš€
